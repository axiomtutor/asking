<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Notes - Axler LADR Ch6</title>
    <link rel="stylesheet" href="../notes.css">
    <script id="MathJax-script" async src="../../../MathJax-master/es5/tex-chtml.js"></script>
  </head>
  <body>
    <p>Throughout this chapter we will assume that the field of scalars is the real or complex numbers.</p>
    <h1>6.A Inner product spaces</h1>
    <p class="def">The <b>dot-product</b> is the sum over coordinate-wise products. An inner-product is a generalization of this, with the properties: <i>positivity, definiteness, left additivity, left homogeneity, and conjugate symmetry</i>.  Examples include the Euclidean inner product, an arbitrary coordinate-wise positive weighting of the dot-product, the definite integral on an interval over the product, and something like the Laplace transform.</p>
    <p class="def">An <b>inner-product space</b> is a vector space with an inner-product.</p>
    <p class="theorem">
      <b>Theorem: </b> An inner-product satisfies the properties <br><br>
      (a) For a fixed <i>u</i> the function of <i>v</i> defined by \(\langle v,u\rangle \) is a linear functional.  <br><br>
      (b) \( \langle 0,u\rangle =0 \) <br>
      (c) \( \langle u,0 \rangle=0\) <br>
      (d) right-additivity <br>
      (e) "twisty" right-homogeneity 
    </p>
    <p class="def">
      The <b>norm</b> of any vector is \(\| \vec{v} \|^2 = \vec{v}\cdot\vec{v} \).  
    </p>
    <p class="theorem">
      <b>Theorem: </b> A norm always satisfies <br> 
      (a) \( \|\vec{v}\| = 0\)  <br>
      (b) \( \|\lambda\vec{v}\| = |\lambda|\|\vec{v}\| \)
    </p>
    <p class="def">Vectors are <b>orthogonal</b> if their product is 0.</p>
    <p class="theorem"><b>Theorem: </b> The Pythagorean Theorem for vectors.</p>
    <p class="proof">
      <b>Proof: </b> We want to show \( \|\vec{u}\|^2+\|\vec{v}\|^2 = \|\vec{u}+\vec{v}\|^2\) if \(\langle\vec{u},\vec{v}\rangle=0\).  This is the same as 
      
      $$\langle\vec{u},\vec{u}\rangle\ +\ \langle\vec{v},\vec{v}\rangle = \langle\vec{u}+\vec{v},\ \vec{u}+\vec{v}\rangle$$
      
      which clearly is equivalent to the claim that 
      
      $$0=\langle \vec{u},\vec{v}\rangle+\langle \vec{v},\vec{u}\rangle$$
      
      and by the assumption of orthogonality, each term on the right is 0.
    </p>
    <p class="note"><b>Note: </b> Below I use the same symbols to mean different things from how Axler chose his symbols' meanings.</p>
    <p class="def">
      We now build up the Gram-Schmidt procedure starting with two vectors in <i>F<sup>n</sup></i> with <i>n</i>>1.  This is just the <b>orthogonal projection</b> of one vector onto another, which we can generalize to projection of a vector onto a space, which gives an algorithm for generating an ortho-normal basis.  The orthogonal projection of \(\vec{u}\) onto \(\vec{v}\) is the vector \(\vec{w}\) such that \(\vec{w}\) is: (a) parallel to \(\vec{v}\) (i.e. \(\vec{w}=c\vec{v}\) ) and (b) the vector \(\vec{u}-\vec{w}\) is orthogonal to \(\vec{v}\).  Solving for <i>c</i> in the equation
      
      $$\langle \vec{u}-c\vec{v}, \vec{v} \rangle = 0$$
      
      for known vectors \(\vec{u},\vec{v}\) is a simple matter and results in 
      
      $$c = \frac{\langle \vec{u},\vec{v}\rangle}{\|\vec{v}\|^2} \quad \Rightarrow \quad \vec{w} = \frac{\langle\vec{u},\vec{v}\rangle}{\|\vec{v}\|^2}\vec{v}$$
    </p>
    <p>
      If we next have three vectors, call them \(\vec{v}_1, \vec{v}_2,\vec{v}_3\in F^n\), suppose we state a new goal but one that is related to the previous goal:  We would like to have three vectors orthogonal to each other and spans the same space that \(\vec{v}_1, \vec{v}_2,\vec{v}_3\) spans.  We assume now that <i>n</i>>2 so that such a goal is possible.  
    </p>
    <p>We can proceed in steps:  If we take \(\vec{v}_1\) to start it is trivially orthogonal to all other vectors because there are no other vectors.  If we add \(\vec{v}_2\) then the set is no longer orthogonal but we may get the orthogonal projection \(\vec{w}\) as above and then add \(\vec{v}_2-\vec{w}\) to the set.  If we call this \(proj_{\vec{v}_1}(\vec{v}_2)\) then the set \(\{v_1,proj_{\vec{v_1}}(\vec{v_2})\}\) will be orthogonal and span the same space as \(\{\vec{v_1},\vec{v_2}\}\).  
    </p>
    <p>
      Next we want to follow a similar procedure for \(\vec{v}_3\).  That means we want to find the projection of \(\vec{v}_3\) onto the space spanned by the first two vectors. This will then allow us to compute the difference between the projection and the original vector to get an orthogonal vector spanning the same space.  The logic of projecting a vector onto a space is similar to that of projecting onto a single vector.  We want \(\vec{w}\) such that (a) we have \(\vec{w}\) is a linear combination of \(\vec{v}_1,\vec{v}_2\) and (b) we have \(\vec{v}_3-\vec{w}\) is orthogonal to both \(\vec{v}_1,\vec{v}_2\). This implies solving a system of two equations, 
      
      $$ \langle \vec{v}_3-(a\vec{v}_1+b\vec{v}_2), v_i \rangle= 0, \quad i = 1,2$$
      
      for <i>a,b</i>.  
    </p>
    <p>
      The solution you should get is
      
      $$\vec{w} = \frac{\langle \vec{v}_1, \vec{v}_3\rangle}{\|\vec{v}_1\|^2}\vec{v}_1 + \frac{\langle \vec{v}_2, \vec{v}_3\rangle}{\|\vec{v}_2\|^2}\vec{v}_2 $$
    </p>
    <p class="def">
      The pattern generalizes to more vectors.  An <b>orthonormal basis</b> is a basis such that every two vectors are orthogonal and unit length.  Given any basis, we can use this procedure to find an orthonormal basis, and we call this procedure the <b>Gram-Schmidt procdure</b>.  If the original basis is \(\{\vec{v}_1, ..., \vec{v}_n\}\) then we start with \(\vec{u}_1=\frac{\vec{v}_1}{\|\vec{v}_1\|}\).  Then for each <i>i=</i>2,...,<i>n</i> we can choose a projection vector 
      
      $$\vec{w}_i = \langle \vec{u}_1,\vec{v}_i\rangle \vec{v}_1 + ... + \langle \vec{u}_{i-1},\vec{v}_i\rangle \vec{u}_{i-1}$$
      
      From the projection vector we define 
      
      $$\vec{v}_i^{\bot} = \vec{v}_i - \vec{w}_i$$
      
      and upon making it unit length 
      
      $$\vec{u}_i = \frac{\vec{v}_i^\bot}{\|\vec{v}_i^\bot\|}$$
      
      Then the collection \(\{\vec{u}_1, ..., \vec{u}_n\}\) is orthonomal and spans the same space as the original vectors.  
    </p>
    <p class="theorem">
      <b>Theorem: </b> The Cauchy-Schwartz Inequality $$|\langle \vec{u},\vec{v}\rangle|\leq \|\vec{u}\|\|\vec{v}\|$$
    </p>
    <div class="proof">
      <p>
        <b>Proof: </b> This is an application of the Pythagorean Theorem. 
      </p>
      <p>
        If \(\vec{v}=\vec{0}\) the theorem is trivial.  Otherwise get 
        
        $$ \vec{u} = \vec{u}^{\|}+\vec{u}^\bot = proj_{\vec{v}_1}(\vec{u}) + (\vec{u}-proj_{\vec{v}_1}(\vec{u}))$$
        
        and consider squared magnitudes <i>a la</i> the Pythagorean Theorem
        
        $$ \|\vec{u}\|^2 = \|\vec{u}^\|\|^2 + \|\vec{u}^\bot\|^2 $$
        
        which delivers the intuitively obvious fact that the projection is never larger than the original vector:
        
        $$ \|\vec{u}\|^2 \geq \|\vec{u}^\|\|^2 $$
        
        Expanding definitions and light rearrangement yields the <i>coup de gras</i>.
      </p>
    </div>
    <p class="theorem">
      <b>Theorem: </b> The Triangle Inequality 
      $$ \|\vec{u}+\vec{v}\|\leq \|\vec{u}\|+\|\vec{v}\|$$
    </p>
    <div class="proof">
      <p>
        <b>Proof: </b> Prove the squared version of this, using Cauchy-Schwartz.
      </p>
      <p>
        To do so, expand the left in terms of inner-products.  Group terms in the obvious way and use \(\langle \vec{u},\vec{v}\rangle + \overline{\langle\vec{u},\vec{v}\rangle} = 2\Re\{\langle\vec{u},\vec{v}\rangle\}\).  Then use the fact that any real number is at least as big as its absolute value, then apply the result from earlier.
      </p>
    </div>
    <p class="theorem">
      <b>Theorem: </b> The Parallelogram Equality
      $$\|\vec{u}+\vec{v}\|^2 + \|\vec{u}-\vec{v}\|^2 = 2(\|\vec{u}\|^2+\|\vec{v}\|^2)$$
    </p>
    <div class="proof">
      <p>
        <b>Proof: </b> This falls right out of definitions of inner-products.
      </p>
    </div>
    <h1>6.B Orthonormal Bases</h1>
    <p>By doing things out of the book's order I've already described what an orthonormal basis is and how to get it.  The value of orthonormal bases are in the following theorems.</p>
    <p class="theorem">
      <b>Theorem: </b> Magnitudes in orthonormal bases obey the Pythagorean Theorem.
      $$ \left\|\sum a_i\vec{e}_i\right\|^2 = \sum |a_i|^2 $$
    </p>
    <p class="proof">
      Repeated application of the Pythagorean Theorem.
    </p>
    <p>The above makes it easy to prove that orthogonal vectors are independent.</p>
    <p class="theorem"><b>Theorem: </b> Orthogonal vectors are independent.</p>
    <div class="proof">
      <p>
        $$a_1\vec{v}_1+...+a_n\vec{v}_n = \vec{0}$$
        implies that the magnitude on the left is 0.  We have for orthogonal vectors that this magnitude squared (also 0) is the same as 
        $$\sum \|a_i\vec{v}_i\|^2$$
      </p>
      <p>If these vectors are non-zero then the coefficients must all be 0.</p>
    </div>
    <p>Therefore orthogonal spanning vectors form an orthogonal basis.</p>
    <p class="theorem">
      <b>Theorem:</b> The magnitude of any vector in an orthonormal basis is 
      
      $$\|\vec{v}\|^2 = \sum |\langle \vec{v},\vec{e}_i\rangle|^2$$
    </p>
    <p class="proof"><b>Proof: </b> This says essentially the same thing as the magnitude being determined by the Pythagorean Theorem since the coordinates of \(\vec{v}\) just are the \(\langle \vec{v},\vec{e}_i\rangle\)</p>
    <p class="theorem"><b>Theorem: </b> If a transformation is triangulable then it is triangulable w.r.t. some orthonormal basis.</p>
    <div class="proof">
      <b>Proof:</b> Suppose <i>T</i> is upper triangular in basis \(\vec{v}_1,...,\vec{v}_n\) and therefore by 5.26 the span is invariant under <i>T</i>.  Let \(\vec{e}_1, ..., \vec{e}_n\) be the product of Gram-Schmidt.  They span the same space and therefore the span of the orthonormal span is invariant under <i>T</i> and therefore the matrix of <i>T</i> is upper triangular in it.
    </div>
    <p class="def">A <b>linear functional</b> which we denote \(\varphi\) is any function $$\varphi : V \rightarrow F$$ where <i>F</i> is the underlying field.</p>
    <p class="theorem">
      <b>Theorem:</b> Riesz Representation Theorem <br><br>
      Every linear function acts like the inner-product with an appropriate choice of fixed vector.
    </p>
    <div class="proof">
      <p><b>Proof:</b> If (varphi) is any linear functional, first let (vec{e}_1,...,vec{e}_n) be an orthonormal basis for <i>V</i>.  If (vin V) then by expanding in its coordinates as inner-products with the basis, and then factoring, it is easy to show 
        
        $$ \varphi(\vec{v}) = \langle \vec{v}, \overline{\sum \varphi(\vec{e}_i)\vec{e}_i}\rangle $$</p>
      </p>
      <p>Note that expansion of a vector's coordinates in terms of inner products with the bases is not so simple when the basis is not orthonormal.</p>
    
      <p>Now to prove uniqueness, we hope to show that if \(\varphi(\vec{v}) = \langle \vec{v}, \vec{u}_1\rangle = \langle \vec{v},\vec{u}_2\rangle \) for every choice of \(\vec{v}\), then \(\vec{u}_1 = \vec{u}_2\). If the assumption holds then 
      $$\langle \vec{v},\vec{u}_1-\vec{u}_2\rangle = 0$$
      
      Since this holds for all \(\vec{v}\) then choose \(\vec{u}_1-\vec{u}_2\) so that we get the norm.  And so on.
      </p>
    </div>
    <h2>Exercises 6.B</h2>
    <ol>
      <li>
        <ol type="a">
          <li> Easy. </li>
          <li> 
            Show that every orthonormal basis for \(\mathbb{R}^2\)is of the form \(\left\{\begin{bmatrix}\cos\theta\\ \sin\theta\end{bmatrix},
            \begin{bmatrix}-\sin\theta\\ \cos\theta\end{bmatrix}\right\}\) or its swap.
            <div class="sol">
              <p>
                <b>Solution:</b> 
                Certainly any unit vector must have the form \( \begin{bmatrix}\cos\theta\\ \sin\theta\end{bmatrix}\) (proof shouldn't be too hard).  Then we seek a second vector \( \begin{bmatrix}a\\ b\end{bmatrix}\) satisfying 
                
                $$a^2+b^2=1 $$$$ a\cos\theta + b\sin\theta = 0$$
                
                For all but a few cases, this implies 
                
                $$a = -b\tan\theta$$
                
                so 
                
                $$(-b\tan\theta)^2+b^2 = 1 \quad \Rightarrow $$
                
                $$b^2 = \frac{1}{\tan^2\theta+1} = \cos^2\theta$$
                
                From here the rest is straight-forward.
              </p>
            </div>
          </li>
        </ol>
      </li>
      <li>
        If \(\vec{e}_1,...,\vec{e}_m\) is an orthonormal collection, \(\vec{v}\in V\), then prove:
        
        $$\|\vec{v}\|^2 = \sum |\langle\vec{v},e_i\rangle|^2$$
        
        iff \(\vec{v}\in span(\vec{e}_i)\).
        
        <div class="sol">
          <p>
            <b>Solution: </b> 
            Suppose \(\vec{v}\in span(\vec{e}_1,...,\vec{e}_m)\) then it is easy to show \(\vec{v} = \sum a_i\vec{e}_i\) where \(a_i=\langle \vec{v},\vec{e}_i\rangle\) and the result follows quickly.  
          </p>
        </div>
      </li>
    </ol>
    
    
    
    
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </body>
</html>

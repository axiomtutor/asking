<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes - Rudin Principles Ch5</title>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../../../css/notes.css">
  </head>
  <body>
    <h1>Chapter 5</h1>
    <h1>Differentiation</h1>

    <h2>Section 1</h2>
    <h2>The Derivative of a Real Function</h2>

    <div>
      <p>
        If <i>f</i> is a real-valued function on [<i>a,b</i>] then for any \(x\in [a,b]\) we define the <b>derivative at <i>x</i></b> to be the limit of the difference quotient as \(t\to x\).
      </p>
      <h3>Theorem: Differentiability implies continuity.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> We want to show that the limit of the function equals the function value,

          $$ \lim_{t\to x}f(t) = f(x) $$

          which we can rearrange into the difference quotient,

          $$ \lim_{t\to x}(f(t)-f(x)) = 0 $$

          $$ \lim_{t\to x}\left( \frac{f(t)-f(x)}{t-x} \cdot (t-x)\right) = 0 $$
        </p>
        <p>
          Now by our assumption of differentiability, we can distribute the limit to each factor, it becomes \( c\cdot 0 \) for some finite real <i>c</i>.
        </p>
      </div>
      <h3>Theorem: The derivative of a monomial, \(\frac{d}{dx} x^n = nx^{n-1}\)</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The proof can easily be done by induction.  Alternately, one can use the formula

          $$ t^n - x^n = (t-x)\sum_{i=0}^{n-1}t^{n-1-i}x^i $$

          for a direct proof.  These proof techniques only apply for \( n\in\mathbb N \).  We can again prove a base-case for <i>n=-1</i> and then use induction to obtain this result for all \( n\in \mathbb Z\).
        </p>
      </div>
      <h3>Theorem: The algebra of derivatives is true.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> That the derivative of a sum is the sum of derivatives is due to the distributivity of limits.  But to find the derivative of a product we have to find

          $$ \lim_{t\to x}\frac{f(t)g(t)-f(x)g(x)}{t-x} = \lim_{t\to x}\frac{f(t)g(t)-f(t)g(x)+f(t)g(x)-f(x)g(x)}{t-x} $$

          where this equality is motivated by the fact that it affords two factorings, one on the left and one on the right.

          $$ = \lim_{t\to x} \frac{f(t)(g(t)-g(x)) + g(x)(f(t)-f(x))}{t-x} $$
        </p>
        <p>
          Now distribute the divisor, then the limit, you get the product rule.
        </p>
        <p>
          The quotient rule follows from the product and chain rules, together with the derivative of a monomial, so we prove the chain rule next.
        </p>
      </div>
      <h3>Theorem: The chain rule, \(\frac{d}{dx}f(g(x))=f'(g(x))g'(x) \).</h3>
      <div class="sol question">
        <p>
          <i>Explanation:</i> We need find the limit

          $$ \lim_{t\to x} \frac{f(g(t)) - f(g(x))}{t-x} $$

          A traditionally desired, and nearly correct proof, is to identify that <i>g(t)</i> and <i>g(x)</i> are "the variables" of <i>f</i>, so to get the derivative of <i>f</i> we work these in as if they were the <i>t</i> and <i>x</i> alone.

          $$ =\lim_{t\to x} \frac{f(g(t))-f(g(x))}{t-x}\cdot\frac{g(t)-g(x)}{g(t)-g(x)} = \lim_{t\to x} \frac{f(g(t))-f(g(x))}{g(t)-g(x)}\cdot \frac{g(t)-g(x)}{t-x}$$
        </p>
        <p>
          At this point we are itching to distribute the limit and say that the left is <i>f'(g(x))</i> and the right is <i>g'(x)</i>. Unfortunately, this could be invalid.  <i>g(t)-g(x)</i> could be zero.  In most other cases we are protected from this due to how the limit is defined. Namely, limits by definition specifically omit the choice <i>t=x</i> (and that way of defining the limit is largely motivated by precisely this reason).  But since here we no longer have this guarantee, we must be more careful.
        </p>
        <p>
          So we need to consider two possibilities: We could pick points as \(t\to x\) such that \(g(t)-g(x)\ne 0\), in which case we can use the sequence theorems about limits.  In this case, we are now assured that our itching desire to distribute limits is valid, and the proof from here is easy.  If no such sequence exists, i.e. every sequence as \(t\to x\) must contain some points at which \(g(t)-g(x)=0\) then it must be the case that in fact <i>g(x)</i> is constant on some interval around <i>x</i>.  In that case <i>f(g(x))</i> is constant on the same interval and the derivative must be 0.  In this case the chain rule holds as a special case, as <i>g'(x) = 0</i>.
        </p>
        <p>
          Now in fact Rudin gives an alternate proof that is also elegant and worth-while because it uses a principle we'll later have use for.  I like to start of the introduction of this principle by considering a somewhat natural object, the linearization of <i>f</i>, which we presumably remember from calculus is defined as

          $$ L(x) = f'(a)(x-a)+f(a) $$

          This is also the tangent line to the graph, and is the first-order approximation of <i>f</i>.  As an approximation, it's natural to also study the error, which we could call <i>e</i> and therefore define as

          $$ e(x) = f(x) - L(x) $$
        </p>
        <p>
          Since <i>L</i> is a first-degree polynomial and <i>f</i> is differentiable (therefore continuous), then <i>e</i> is continuous.  Also clearly \(\lim_{x\to a}e(x)=0\).  If we then solve for <i>f</i> we can say that

          $$ f(x) = e(x) + L(x) $$
        </p>
        <p>
          Now at this point it becomes increasingly important to note that the linearization is centered at <i>a</i>.  So to not switch around notation yet again, we will persist with this notation for the center.  Therefore, with this notation for the overall proof, our goal will be to analyze

          $$ \lim_{t\to a} \frac{f(g(t))-f(g(a))}{t-a} $$
        </p>
        <p>
          Let the functions <i>L<sub>g</sub></i> and <i>e<sub>g</sub></i> be as above where

          $$ g(x) = L(x)+e(x) = g'(a)(x-a)+g(a)+e(x) $$

          Then we can write

          $$ g(t)-g(a)=g'(a)(t-a)+e(t)+e(a) = g'(a)(t-a)+e(t) $$

          Similarly there is a second function such that

          $$ f(s)-f(g(a)) = f'(g(a))(s-g(a))+e_2(s) $$

          As a final "letting", we let <i>s=g(t)</i>.  Since we will eventually take a limit let us immediately note that \(\lim_{t\to a}e_2(s) = e_2(\lim_{t\to a}s) = e_2(g(a))=0\).  Then

          $$ \frac{f(g(t)) - f(g(a))}{t-a} = \frac{ f'(g(a))(s-g(a)) + e_2(s) }{t-a} $$
          $$ = \frac{f'(g(a))(g(t)-g(a))+e_2(s)}{t-a} = \frac{f'(g(a))(g'(a)(t-a)+e(t))+e_2(s)}{t-a} $$
        </p>
      </div>
    </div>

    <h2>Section 5.2</h2>
    <h2>Mean Value Theorems</h2>

    <div>
      <p>
        In any metric space, we say that the point <i>p</i> is a local max for the real function <i>f</i> if there is a neighborhood in which <i>f(p)</i> dominates.
      </p>
      <h3>Theorem: For a function on a closed real interval, if the derivative exists at a local max, then the derivative is zero.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Of course we inspect the difference quotient on each side.  To the left it's always positive, and since the limit exists, it cannot be less than zero.  Conversely on the right, it cannot be greater than zero.
        </p>
      </div>
      <h3>Theorem: If <i>f, g</i> are continuous real functions on [<i>a, b</i>] and differentiable on (<i>a, b</i>) then there is a point \(x\in (a,b)\) at which

      $$ [f(b)-f(a)]g'(x) = [g(b)-g(a)]f'(x) $$
       </h3>
       <div class="sol">
         <p>
           <i>Explanation:</i> Where does this equation come from?  One answer is that, so long as relevant quantities are not zero, it's equivalent to

           $$ \frac{f(b)-f(a)}{g(b)-g(a)} = \frac{f'(x)}{g'(x)} $$

           This is the change in <i>f</i> over the change in <i>g</i>.  On the left that is understood to mean at the end-points, representing the net change for each function.  On the right this is at a single point in-between, using the derivatives.  This is evocative of the traditional "mean value theorem", but generalized from the case where <i>g</i> is the identity function.
         </p>
         <p>
           A common technique when proving an equality in analysis is to try to pack everything to one side, so that it is a single function.  Then the goal is to prove that this is zero.  Taking the equation in the statement of the theorem, we want to show that \(h(t)=[f(b)-f(a)]g(t)-[g(b)-g(a)]f(t) \) has a derivative equal to zero somewhere (in (<i>a,b</i>)).
         </p>
         <p>
           Recalling Rolle's theorem, if we want the derivative to be zero it is enough to know that the end-points take the same value.  We haven't yet proved Rolle's but it will be a simple matter shortly.  And we can easily check that indeed <i>h(a) = h(b)</i>.
         </p>
         <p>
           To prove Rolle's theorem we just need that <i>f</i> increases on some interval and then the extreme value theorem dictates that it obtains a maximum inside the interval.  Eliminating the possibility of the max being at an endpoint is simple.  Then we can infer the maximum is a local maximum, so the derivative is zero by the previous theorem.  If it doesn't increase, then maybe it decreases.  If it does neither, it's constant, and the derivative is zero everywhere.
         </p>
       </div>
       <h3>Theorem: A real differentiable function on (<i>a,b</i>) with non-negative derivative is montonically increasing.  If the derivative is zero the function is constant.</h3>
       <div class="sol">
         <p>
           <i>Explanation:</i> For any <i>x<sub>1</sub> < x<sub>2</sub></i> in the interval, we can apply the above to infer <i>f(x<sub>2</sub>)-f(x<sub>1</sub>) = (x<sub>2</sub>-x<sub>1</sub>)f'(x)</i> for some \(x\in (a,b)\), which immediately implies the desired result.
         </p>
       </div>
    </div>

    <h2>Section 5.3</h2>
    <h2>The Continuity of Derivatives</h2>

    <div>
      <h3>Theorem: If a function is a derivative then it has the intermediate value property.</h3>
      <div class="sol question">
        <p>
          <i>Explanation:</i> One half of the claim is the following. If <i>f</i> is a real function on the interval [<i>a,b</i>] and is differentiable there, and if \(f'(a) < \lambda < f'(b)\), then there exists some \(x\in (a,b)\) such that \(f'(x)=\lambda\).  The other half reverses one of the inequalities, and the proof is the same.
        </p>
        <p>
          As we've seen before it may be useful to pack everything to one side and restate the problem as finding an <i>x</i> such that \(\frac{d}{dt}(f(t)-\lambda t)\) becomes 0 at <i>x</i>.  But clearly \(f(t)-\lambda t\) is continuous and therefore has the extreme value property. We would like to now conclude that <i>g</i> has an extreme value, and use the fact that the derivative is zero at an extrema.
        </p>
        <p>
          However, we must be careful since the extreme value may be at <i>a</i> or at <i>b</i> where the derivative may not exist and so may not be zero.  We instead need an extrema in (<i>a,b</i>).  We can guarantee that this exists because the assumptions guarantee <i>g'(a) < 0</i> which ensures there is some point <i>t<sub>1</sub></i> such that <i>g(t<sub>1</sub>) < g(a)</i>.  So <i>g(a)</i> is not the minimum.  A similar argument shows <i>g(b)</i> is not the minimum, hence the minimum is in the interior, and hence the derivative there is zero.
        </p>
      </div>
      <h3>Theorem: Functions which are derivatives do not have simple discontinuities.</h3>
      <div class="sol question">
        <p>
          <i>Explanation:</i> If a derivative had a simple discontinuity then at that discontinuity the left- and right-limits would be distinct and in a neighborhood the function must fail the intermediate value property.
        </p>
      </div>
    </div>

    <h2>Section 5.4</h2>
    <h2>L'Hospital's Rule</h2>

    <div>
      <h3>Theorem: L'Hospital's Rule is true.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> We usually invoke this when we are trying to determine

          $$ \lim_{x\to a} \frac{ f(x) }{ g(x) } $$

          but can't because either <i>f, g</i> both go to 0 or both go to &infin;.  So then we are told we may differentiate both and then try again. Let's now make sure the logic behind what we're actually doing is clear, and start with the case that <i>f, g &rightarrow; 0</i> as <i>x&rightarrow;a</i>.  We in fact need to prove that if \(\lim_{x\to a} \frac{ f'(x) }{ g'(x) } = A\) then the solution to our original problem is also \(\lim_{x\to a} \frac{ f(x) }{ g(x) } = A\).
        </p>
        <p>
          So with all that in mind, we assume <i>lim<sub>x&rightarrow;a</sub>f(x) = lim<sub>x&rightarrow;a</sub>g(x) = 0</i> and <i>lim<sub>x&rightarrow;a</sub><sup>f '(x)</sup>/<sub>g'(x)</sub> = A</i>.  The overall strategy will be to show two inequalities for <i>lim<sub>x&rightarrow;a</sub><sup>f '(x)</sup>/<sub>g'(x)</sub></i>.  All along we will assume that <i>A</i> and <i>a</i> are finite real numbers, but the proof generalizes as well to the case when <i>A, a = &pm;&infin;</i>.
        </p>
        <p>
          To prove first that <i>lim<sub>x&rightarrow;a</sub><sup>f &prime;(x)</sup>/<sub>g&prime;(x)</sub> &leq; A</i> we will show that for any <i>A < q</i> there is a <i>c</i> such that the region <i>a < x < c</i> requires <i>f(x)/g(x) < q</i>.  Since <i>A < q</i> is chosen arbitrarily, this entails the desired inequality.  To this end let <i>q</i> be so chosen, and in fact we will choose an even smaller value <i>A < r < q</i> because later we will arrive at an inclusive inequality when what we want is a strict inequality.
        </p>
        <p>
          Now by <i>lim<sub>x&rightarrow;a</sub><sup>f &prime;(x)</sup>/<sub>g&prime;(x)</sub> = A</i> this entails the existence of a <i>c</i> such that for all <i>a < x < c</i> we have <i><sup>f &prime;(x)</sup>/<sub>g&prime;(x)</sub> < r</i>. The ratio of derivatives looks like the generalized mean value theorem, so let's try leveraging that.  We take any <i>a < x < y < r</i> and can infer

          $$ \frac{f(x) - f(y)}{g(x) - g(y)} = \frac{f'(t)}{g'(t)} < r $$

          for some <i>t &in; (x,y)</i>.  Now with <i>y</i> fixed we can see that as <i>x&rightarrow;a</i> we get

          $$ \frac{f(y)}{g(y)} \leq r $$

          Now recall that <i>y</i> was chosen arbitrarily in <i>x < y < c</i>, so since we have <i>x&rightarrow;a</i> then this holds for <i>a < y < c</i>.  So for all such <i>y</i>

          $$ \frac{f(y)}{g(y)} < q $$

          which delivers on the goal stated at the start.
        </p>
        <p>
          The proof for the other direction of the inequality, i.e. <i>lim<sub>x&rightarrow;a</sub><sup>f &prime;(x)</sup>/<sub>g&prime;(x)</sub> &geq; A</i>, is the same.  And the various generalizations to <i>A, a = &pm;&infin;</i> are very similar.
        </p>
      </div>
    </div>

    <h2>Section 5</h2>
    <h2>Derivatives of Higher Order</h2>

    <div>
      <p>
        We write <i>f<sup>(n)</sup>(x)</i> for the <b><i>n</i>th derivative</b>.
      </p>
    </div>

































  </body>
</html>

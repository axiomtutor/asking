<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes - Rudin Principles Ch3</title>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../notes.css">
  </head>
  <body>
    <h1>Chapter 3</h1>
    <h1>Numerical Sequences and Series</h1>

    <h2>Section 1</h2>
    <h2>Convergent Sequences</h2>

    <div>
      <p>
        A sequence \(\{p_n\}\) is said to <b>converge</b> if there is a point \(p\in X\) such that \((\forall \varepsilon\in \mathbb R^+) (\exists N\in \mathbb Z^+) (\forall n\in \mathbb{Z}^+, n \geq N)( d(p_n,p) < \varepsilon)\).  We write either \(p_n\rightarrow p\) or \(\displaystyle \lim_{n\rightarrow \infty}p_n = p\).
      </p>
      <h3>Theorem: A sequence converges to a point if and only if every neighborhood of the point excludes only finitely many points in the sequence.  Limits are unique.  Convergence implies bounded.  Limit points of sets have sequences converging to the point, with the sequence in the set.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Proving the first sentence, we assume a sequence converges.  Then for each neighborhood with a given radius, there exists a corresponding <i>N</i>.  By definition only the terms before this could be outside the neighborhood.  For the reverse direction, the argument is similar.
        </p>
        <p>
          <i>That limits are unique:</i> Take any two limits and show that their distance from each other is smaller than every positive \(\varepsilon\).  To do this, find an <i>N</i> corresponding to \(\varepsilon/2\) for each limit point and use the triangle inequality.
        </p>
        <p>
          <i>That convergence implies bounded:</i> Use \(\varepsilon = 1\).  The terms after this are all less than \(p+\varepsilon\), thinking of a positive example (negatives are handled just as easily).  Terms before it are finite and must have a max.
        </p>
        <p>
          <i>That limit points of sets have a sequence in the set which converges to it:</i> Take a neighborhood, pick a point, take a smaller neighborhood ... Construct your sequence this way.
        </p>
      </div>
      <h3>Theorem: If individual limits exist then limits are linear, distribute over multiplication, and into denominators (except limit 0).</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> For distributing over sums, pick \(\varepsilon/2\) and use the triangle inequality.  For distributing over constants, use \(\varepsilon/c\).  Distributing over products is harder, so we focus on that.
        </p>
        <p>
          <i>That limits distribute over products:</i> Wiggle into \(|s_nt_n - st|\) the term \(s_nt\).  Then we need a bound on <i>n</i> which guarantees

          $$ |s_nt_n - s_nt + s_nt t_n - st| < \varepsilon $$
        </p>
        <p>
          Use the triangle inequality

          $$ |s_n t_n - s_n t + s_n t - st| \leq |s_nt_n - s_n t| + |s_n t - st| $$
          $$ = |s_n||t_n-t| + |t||s_n-s| $$
        </p>
        <p>
          Now we can easily get a bound on the right-hand term because <i>t</i> is fixed.  We can enforce a condition on the <i>s</i>'s in terms of \(\varepsilon\) and <i>t</i>.  The tricky part is the left-hand term because \(|s_n|\) is not fixed, it's a function of <i>n</i>.  When using the convergence of \(s_n\) we always need bounds that are constant in <i>n</i>.
        </p>
        <p>
          So the trick is to round it up to its maximum, since we already know it's bounded.
        </p>
        <p>
          <i>That limits distribute to the denominator: </i> We need to show \(\Big| \frac 1 {s_n}  - \frac 1 s \Big| < \varepsilon\).

          $$ \Big| \frac{1}{s_n} - \frac 1 s \Big| = \Big| \frac{s-s_n}{ss_n}\Big| $$

          And since the numerator can be bounded, that part is easy.  And the <i>s</i> in the denominator is constant, so not a worry.  But again the \(s_n\) is a cause for concern but this time it's in the denominator.  And rounding up to the upper bound won't help since it moves the inequality in the wrong direction.
        </p>
        <p>
          But actually it is somewhat clear that these old strategies won't work this time.  We need to use the assumption that the limit is not zero.  So find an <i>N</i> such that the sequence stays far enough away from zero.  For specificity, let <i>N</i> be such that \((\forall n\geq N)(|p_n-p| < p/2) \).  This again assumes <i>p > 0</i> but the proof for a negative <i>p</i> is very similar.  What we're effectively doing here is just choosing the half-way point between 0 and <i>p</i>.  With all the terms staying inside that region, we can then replace \(p_n\) with the lower bound <i>p/2</i>.
        </p>
      </div>
      <div>
        <h3>Theorem: Vector sequences converge if and only if each coordinate does.  They also obey distribution in sums and dot-products.  Also if \(\vec x_n \rightarrow \vec x, \beta_n \rightarrow \beta\) then \(\displaystyle \lim_{n\rightarrow \infty} \beta_n\vec x_n = \beta\vec x\).</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> If every coordinate convergences, then to make \(|\vec x_n - \vec x | < \varepsilon \) that means we need

            $$ \sqrt{(x_{1,n}-x_1)^2 + \dots + (x_{n,n}-x_n)^2} < \varepsilon $$
          </p>
          <p>
            We want to do a term-by-term rounding up of the LHS, and we want to replace each \(x_{i,n}-x_i\) by \(\varepsilon\) or at least something close (maybe divided by <i>n</i>, or after toying with that it might be even better to use \(\sqrt n\).)  Lucky for us the assumption is just right for this sort of thing.
          </p>
          <p>
            To go in the other direction is even easier just because the absolute value of any coordinate is always smaller than the absolute value of a vector.
          </p>
          <p>
            This then allows us to easily prove the rest coordinate-wise.
          </p>
        </div>
      </div>
      <div>
        <p>
          A <b>subsequence</b> is any in-order subset of the sequence.  If a subsequence has a limit we call it at subsequential limit of the larger sequence.
        </p>
        <h3>Theorem: Sequences in compact sets always have a subsequential limit in the set. Every bounded sequence in has a subsequential limit.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> There are two important parts of this claim.  First, these sequences always have subsequential limits.  Second, that the limit is in the set.  Boundedness would already be enough to guarantee that the sequence has some subsequential limit.  Closedness guarantees that the set contains its limit points.
          </p>
          <p>
            First notice that, as soon as the first part is done we can quickly handle the second by embedding any bounded set in a <i>k</i>-cell.
          </p>
          <p>
            Now to prove the first part, we consider whether the sequence visits finitely or infinitely many points.  If it's finite the matter is trivial: Some point must be visited infinitely often since there are infinitely many indices.
          </p>
          <p>
            If it visits infinitely many points, then this sounds too much like previous theorems about infinite subsets and compactness.  Once you obtain a limit point it is easy to use it to construct a sequence approaching it.
          </p>
        </div>
      </div>
      <div>
        <h3>Theorem: The set of subsequential limits of a sequence is closed.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> The proof is direct from the definition of closed.  We select a point in the set and show that it is a limit point.  We have to care for some details, but the essential plan is to form a sequence of neighborhoods around the point which shrink to zero, picking elements of the sequence along the way.
          </p>
        </div>
      </div>
    </div>
    <h2>Section 2</h2>
    <h2>Cauchy Sequences</h2>
    <div>
      <p>
        A sequence is <b>Cauchy</b> if \((\forall \varepsilon\in\mathbb R^+)(\exists N\in \mathbb Z^+)(\forall n\in \mathbb Z^+,n\geq N)(d(p_n,p_m)< \varepsilon)\).  This effectively means the points get infinitely close to each other.  In a sense, convergence specifies what the limit point is; on the other hand, Cauchy does not name a point to which the sequence converges. Note also that there may not be a limit point.  You can have Cauchy sequences of rational numbers which "converge to a real", although if viewed entirely within the rationals then you can't actually speak about the reals.  So the more faithful statement would be that there are Cauchy sequences of rationals which do not have any limit.
      </p>
      <p>
        We call the <b>diameter</b> the sup over all the distances in a set.  Viewed geometrically in the plane it's like taking the smallest circle that encompasses all the points, and using its diameter.
      </p>
      <p>
        Unpacking definitions it is not a hard proof that a sequence is Cauchy if and only if the limit of the truncated sequence diameters goes to 0.  Said a little formally, a sequence is Cauchy if and only if

        $$ \displaystyle \lim_{N\rightarrow \infty} diam E_N = 0 $$

        where \(E_N\)  is the set of all points in the sequence after index <i>N</i>.
      </p>
      <div>
        <h3>Theorem: Taking the closure of a set doesn't change its diameter.  Any descending chain of compact sets with diameter approaching 0 consists of exactly one point.</h3>
        <div class="sol">
          <p>
              <i>Expalantion: </i> It's clear that adding more points can only increase the diameter, i.e. \( diam E \leq  diam \overline E\).  To prove the other direction, we take any two points in the closure and show and show that their distance cannot be more than diam<i>E</i>. In particular we show that they are each as close as you want to points in <i>E</i>, so you can find an upper bound on diam\(\overline E\) in terms of \(\varepsilon\) and diam<i>E</i>.  Since \(\varepsilon\) can be chosen arbitrarily small this implies \( diam\overline E\leq diam E \).
          </p>
          <p>
            <i>That descending chains of compact sets with diameters converging to zero have a unique element:</i> We already know that the intersection contains a point, so all that remains is to show it's unique with the new condition that the diameters go to zero.  But if there were two elements the diameter couldn't be smaller than their distance.
          </p>
        </div>
      </div>
      <div>
        <h3>
          Theorem: Convergence entails Cauchy.  In a compact space, Cauchy entails convergence.
        </h3>
        <div class="sol">
          <p>
            <i>Explanation:</i> Of course we are meant to think especially of the case of Euclidean spaces.  Since these are compact, this theorem shows that convergence and Cauchy are equivalent in Euclidean spaces.
          </p>
          <p>
            <i>That convergence entails Cauchy:</i> For any \(\varepsilon\) you can keep all the terms this close to the limit, and therefore close to each other.
          </p>
          <p>
            <i>That Cauchy entails convergence in compact spaces:</i> The the theorems just above will be useful.  In particular, we will use the idea of limits of diameters going to zero.  Since the image of the sequence may not be closed, notice that taking the closure of them doesn't change the diameter.  Now we have a descending chain of closed sets in a compact space.  We then know there is a unique element in the intersection, which we can prove is the limit point.
          </p>
        </div>
        <p>
          A space in which every Cauchy sequence converges is <b>complete</b>.
        </p>
        <p>
          A sequence is <b>monotonically increasing</b> if each next term is no less than the last.
        </p>
        <div>
          <h3>Theorem: A monotonically increasing sequence converges if and only if it's bounded.</h3>
          <div class="sol">
            <p>
              <i>Explanation:</i> If it converges we already know it's bounded, so the interesting part is the other direction.  Suppose it's bounded, then you can prove it converges to its supremum.
            </p>
          </div>
        </div>
      </div>
    </div>
    <h2>Section 3</h2>
    <h2>Upper and Lower Limits</h2>
    <div>
      <p>
        The <b>limsup</b> of a sequence is the supremum over the set of all subsequential limits.  This is also called the <b>upper limit</b>.  Similarly for <b>liminf</b>, called the <b>lower limit</b>.  These are respectively often denoted as \(s^*\) and \(s_*\).  (Note some authors define the limsup as the limit of the suprema of all truncated sequences.  These definitions are equivalent and proving this fact is a good exercise.)
      </p>
      <h3>Theorem: The limsup is a subsequential limit, and any point larger than the limsup is an upper bound for some truncated sequence.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> If the sequence is unbounded you can make a subsequence going to infinity in the usual way.  If it's bounded then we already know that the set of subsequential limits is a closed set.  The limsup is a limit point of this set and therefore in the set.
        </p>
        <p>
          For the second part, if we have a value larger than the sup then all the limits are strictly below it and eventually the sequences stay underneath.  For a proof, suppose some limit never stays below <i>x</i>, a contradiction for \(s^*\) comes pretty quickly thereafter.
        </p>
      </div>
      <h3>Theorem: If one sequence is above another, the lower and upper limits preserve the ordering.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Suppose we discuss the upper limits.  Argue that the upper limit of the bigger sequence is an upper-bound on all possible subsequential limits of the lower sequence.
        </p>
      </div>
    </div>
    <h2>Section 4</h2>
    <h2>Some Special Sequences</h2>
    <div>
      <h3>Theorem: If a sequence is squeezed between 0 and a sequence that goes to 0, then it goes to 0.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Rudin calls this a remark but we probably want to be able to prove it.  Note

          $$ |x_n-0| < \varepsilon \quad \Leftrightarrow \quad x_n < \varepsilon $$

          under these assumptions.  A similar fact can be said about the bounding sequence, and we know we can always fit that under any \(\varepsilon\).
        </p>
      </div>
      <h3>Theorem: \(\frac{1}{n^p}=0\) goes to 0 if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i>

          $$ \frac{1}{n^p} < \varepsilon \quad \Leftrightarrow \quad \frac{1}{\varepsilon^{1/p}} < n $$
        </p>
      </div>
      <h3>Theorem: \(\displaystyle \lim_{n\rightarrow \infty}\sqrt[n]{p} = 1\) if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Show that \(\displaystyle \lim_{n\rightarrow\infty} (p^{1/n}-1) = 0\).  After distributing limits, the result follows immediately.  And note that this is a common tactic in proving that a limit is this or that value: Convert the value itself into a limit and then merge limits.
        </p>
        <p>
          To show that \(\displaystyle\lim_{n\rightarrow\infty}(p^{1/n}-1)=0\) we use the theorem at the start of the section.  What will we use to bound this above?  It helps to just write out \(x_n = p^{1/n}-1\) and it is not a crazy idea to solve for <i>p</i>, although I honestly can't motivate this move any better than to just say it's "not crazy".  I guess you just play with it long enough until you stumble onto this.
        </p>
        <p>
          Once you solve for <i>p</i>, you notice that the binomial theorem applies, and we use the approximation that we discovered in the first chapter.

          $$ (1+x_n)^n \geq 1+nx_n $$
        </p>
      </div>
      <h3>Theorem: \(\sqrt[n] n \rightarrow 1\)</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Similar to the previous problem, except when you get to the application of the binomial theorem. The familiar approximation no longer works.  However, we can still approximate it as an upper bound on the second-order term.  (Since every term is zero and so we can drop any that we want.)
        </p>
      </div>
      <h3>Theorem: \(\frac{n^\alpha}{(1+p)^n} \rightarrow 0\) if <i>p > 0</i> and \(\alpha\in\mathbb R\).</h3>
      <div class="sol">
        <p>
          <i>Explanation: </i> This time we don't solve for anything per se, but we can just directly approximate \((1+p)^n\) which is intuitively the hard spot in this problem anyway.  And of course this is also intuitive because it already looks like the binomial theorem.
        </p>
        <p>
          As we saw before we can approximate it down by dropping any terms we want.  If we keep only the <i>k</i>th term let's see what we get.

          $$ \frac{n^\alpha}{(1+p)^n} < \frac{n^\alpha}{\binom n k p^k} $$
        </p>
        <p>
          We can approximate that down even further by approximating up the binomial factor.

          $$ \binom n k = \frac{n!}{(n-k)!k!} = $$
          $$ n(n-1)\cdots (n-k+1)/k! $$

          Clearly we can round this down by changing <i>n-i</i>s to 1s but this approximation, if you try it, is too severe.  The proof will typically fail.  One thought is that we can approximate at least half of them down to <i>n-i</i> to <i>n/2</i> so long as <i>n-i > n/2</i> (which of course, switches from true to false about halfway down the list of factors).  This helps guide us in the choice of <i>k</i>.  We need <i>n/2 > k</i>.  For any such choice of <i>k</i>

          $$ \binom n k > \frac{n^k}{2^kk!} $$

          This effectively rounds the first <i>k</i> factors to <i>n/2</i> and the rest to 1.
        </p>
        <p>
          Then we have

          $$ \frac{n^\alpha}{(1-p)^n} < \frac{n^\alpha}{n^kp^k/(2^kk!)} $$
          $$ = \frac{n^{\alpha-k}2^kk!}{p^k} $$

          If we in particular pick \(\alpha-k < 0\) then \(n^{\alpha-k}\rightarrow 0\) by an earlier theorem.  We know we can pick a <i>k</i> large enough for this, and then pick an <i>n</i> large enough for <i>n/2 > k</i>.
        </p>
      </div>
      <h3>Theorem: \(x^n\rightarrow 0\) if |<i>x</i>| < 1.</h3>
      <div class="sol">
        <p>
          <i>Explanation: </i> Use the theorem above.  With \(\alpha = 0\) we can make any \(x\geq 0\) equal to some \(\frac 1 {1+p}\).  For the case where <i>x < 0</i>, the even terms and the odd terms both go to 0.  So for any \(\varepsilon\in\mathbb R^+\) we can find an <i>N</i> such that for every odd \(n\geq N \) we have \(|x^n| < \varepsilon\) and then this automatically also holds for all even terms after <i>N+1</i>.  So it holds for all \(n\geq N+1\).
        </p>
      </div>
    </div>
    <h2>Section 5</h2>
    <h2>Series</h2>
    <div>
      <p>
        We did all that work on sequences, in part, because we want to take infinite sums (i.e. <b>series</b>) which are defined as the limits of partial sums.  Each <b>partial sum</b> is just the sum up to some finite final term.  The limit is taken over the number of terms.  If the sequence of terms is given by \(\{a_n\}\) then a partial sum up to <i>q</i> is

        $$ \sum_{n=p}^q a_n $$

        for the series starting at <i>p</i>.  Then for a series

        $$ \sum_{n=1}^\infty a_n = \lim_{q\rightarrow\infty} \sum_{n=1}^q a_n $$

        We say that the series converges or diverges if the limit does.
      </p>
      <p>
        Particularly useful is the Cauchy criterion for series, which becomes:

        $$ (\forall \varepsilon\in\mathbb R^+)(\exists N\in\mathbb Z^+)(\forall m,n\in\mathbb Z^+, \text{ and } m,n\geq N)\left(\left|\sum_{k=m}^n a_k\right|<\varepsilon\right) $$
      </p>
      <h3>Theorem: Test for Divergence</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The test for divergence is the theorem that, if a series converges, then the limit of the sequence of terms goes to 0.  That is to say

          $$  \sum_{n=1}^\infty a_n < \infty \Longrightarrow a_n\rightarrow 0 $$

          This is a good name for the theorem because it is a quick and easy way to show that some series diverge (using the contrapositive).  Whenever meeting a new series your first question should be "does the limit of terms go to 0?"  Much of the time the answer is yes, and you've learned nothing.  But in a good number of cases the answer will be no, and you will be able to immediately infer that the series diverges.
        </p>
        <p>
          The proof follows immediately from the Cauchy criterion for converges when selecting <i>m = n</i>.
        </p>
      </div>
      <h3>Theorem: Non-negative Convergence Test.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The non-negative convergence test is a theorem that applies to series of non-negative terms.  For such a series, it must converge if and only if the partial sums are bounded. If the series converges then the partial sums are bounded just from the fact that convergent sequencecs are bounded.  If the partial sums are bounded then they form an increasing (because the terms are non-negative) bounded sequence, and so must converge.
        </p>
      </div>
      <h3>Theorem: Comparison Test.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The comparison test says that if a larger series converges, then the smaller one does; and if a smaller series diverges then the larger one does.  More rigorously, we imagine some given series \(\displaystyle \sum_{n=1}^\infty a_n\).  If \(|a_n|\leq c_n\) for some other terms \(c_n\) and some \(N_0\) and each \(n\geq N_0\), then

          $$ \sum_{n=1}^\infty c_n < \infty \Longrightarrow \sum_{n=1}^\infty a_n < \infty $$
        </p>
        <p>
          It has a dual as well. If a smaller series diverges then the larger one does.  That is to say, if \(a_n\geq d_n\geq 0\) for \(n\geq N_0\), then

          $$ \sum_{n=1}^\infty d_n \not\in \mathbb R \Longrightarrow \sum_{n=1}^\infty a_n \not\in \mathbb R $$
        </p>
        <p>
          The proof of the first part just uses the Cauchy criterion and the triangle inequality on partial sums.  The proof for the second part simply applies the contrapositive to the first part.
        </p>
      </div>
      <h2>Section 6</h2>
      <h2>Series of Nonnegative Terms</h2>
      <h3>Theorem: The Geometric Series</h3>
      <div class="sol">
        <p>
          The geometric series is \(\displaystyle\sum_{n=1}^\infty x^n\) and famously if \( 0\leq x < 1\) then the series equals

          $$ \frac{1}{1-x} $$
        </p>
      </div>

    </div>






























  </body>
</html>

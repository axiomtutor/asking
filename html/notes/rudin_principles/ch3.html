<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes - Rudin Principles Ch3</title>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../notes.css">
  </head>
  <body>
    <h1>Chapter 3</h1>
    <h1>Numerical Sequences and Series</h1>

    <h2>Section 1</h2>
    <h2>Convergent Sequences</h2>

    <div>
      <p>
        A sequence \(\{p_n\}\) is said to <b>converge</b> if there is a point \(p\in X\) such that \((\forall \varepsilon\in \mathbb R^+) (\exists N\in \mathbb Z^+) (\forall n\in \mathbb{Z}^+, n \geq N)( d(p_n,p) < \varepsilon)\).  We write either \(p_n\rightarrow p\) or \(\displaystyle \lim_{n\rightarrow \infty}p_n = p\).
      </p>
      <h3>Theorem: A sequence converges to a point if and only if every neighborhood of the point excludes only finitely many points in the sequence.  Limits are unique.  Convergence implies bounded.  Limit points of sets have sequences converging to the point, with the sequence in the set.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Proving the first sentence, we assume a sequence converges.  Then for each neighborhood with a given radius, there exists a corresponding <i>N</i>.  By definition only the terms before this could be outside the neighborhood.  For the reverse direction, the argument is similar.
        </p>
        <p>
          <i>That limits are unique:</i> Take any two limits and show that their distance from each other is smaller than every positive \(\varepsilon\).  To do this, find an <i>N</i> corresponding to \(\varepsilon/2\) for each limit point and use the triangle inequality.
        </p>
        <p>
          <i>That convergence implies bounded:</i> Use \(\varepsilon = 1\).  The terms after this are all less than \(p+\varepsilon\), thinking of a positive example (negatives are handled just as easily).  Terms before it are finite and must have a max.
        </p>
        <p>
          <i>That limit points of sets have a sequence in the set which converges to it:</i> Take a neighborhood, pick a point, take a smaller neighborhood ... Construct your sequence this way.
        </p>
      </div>
      <h3>Theorem: If individual limits exist then limits are linear, distribute over multiplication, and into denominators (except limit 0).</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> For distributing over sums, pick \(\varepsilon/2\) and use the triangle inequality.  For distributing over constants, use \(\varepsilon/c\).  Distributing over products is harder, so we focus on that.
        </p>
        <p>
          <i>That limits distribute over products:</i> Wiggle into \(|s_nt_n - st|\) the term \(s_nt\).  Then we need a bound on <i>n</i> which guarantees

          $$ |s_nt_n - s_nt + s_nt t_n - st| < \varepsilon $$
        </p>
        <p>
          Use the triangle inequality

          $$ |s_n t_n - s_n t + s_n t - st| \leq |s_nt_n - s_n t| + |s_n t - st| $$
          $$ = |s_n||t_n-t| + |t||s_n-s| $$
        </p>
        <p>
          Now we can easily get a bound on the right-hand term because <i>t</i> is fixed.  We can enforce a condition on the <i>s</i>'s in terms of \(\varepsilon\) and <i>t</i>.  The tricky part is the left-hand term because \(|s_n|\) is not fixed, it's a function of <i>n</i>.  When using the convergence of \(s_n\) we always need bounds that are constant in <i>n</i>.
        </p>
        <p>
          So the trick is to round it up to its maximum, since we already know it's bounded.
        </p>
        <p>
          <i>That limits distribute to the denominator: </i> We need to show \(\Big| \frac 1 {s_n}  - \frac 1 s \Big| < \varepsilon\).

          $$ \Big| \frac{1}{s_n} - \frac 1 s \Big| = \Big| \frac{s-s_n}{ss_n}\Big| $$

          And since the numerator can be bounded, that part is easy.  And the <i>s</i> in the denominator is constant, so not a worry.  But again the \(s_n\) is a cause for concern but this time it's in the denominator.  And rounding up to the upper bound won't help since it moves the inequality in the wrong direction.
        </p>
        <p>
          But actually it is somewhat clear that these old strategies won't work this time.  We need to use the assumption that the limit is not zero.  So find an <i>N</i> such that the sequence stays far enough away from zero.  For specificity, let <i>N</i> be such that \((\forall n\geq N)(|p_n-p| < p/2) \).  This again assumes <i>p > 0</i> but the proof for a negative <i>p</i> is very similar.  What we're effectively doing here is just choosing the half-way point between 0 and <i>p</i>.  With all the terms staying inside that region, we can then replace \(p_n\) with the lower bound <i>p/2</i>.
        </p>
      </div>
      <div>
        <h3>Theorem: Vector sequences converge if and only if each coordinate does.  They also obey distribution in sums and dot-products.  Also if \(\vec x_n \rightarrow \vec x, \beta_n \rightarrow \beta\) then \(\displaystyle \lim_{n\rightarrow \infty} \beta_n\vec x_n = \beta\vec x\).</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> If every coordinate convergences, then to make \(|\vec x_n - \vec x | < \varepsilon \) that means we need

            $$ \sqrt{(x_{1,n}-x_1)^2 + \dots + (x_{n,n}-x_n)^2} < \varepsilon $$
          </p>
          <p>
            We want to do a term-by-term rounding up of the LHS, and we want to replace each \(x_{i,n}-x_i\) by \(\varepsilon\) or at least something close (maybe divided by <i>n</i>, or after toying with that it might be even better to use \(\sqrt n\).)  Lucky for us the assumption is just right for this sort of thing.
          </p>
          <p>
            To go in the other direction is even easier just because the absolute value of any coordinate is always smaller than the absolute value of a vector.
          </p>
          <p>
            This then allows us to easily prove the rest coordinate-wise.
          </p>
        </div>
      </div>
      <div>
        <p>
          A <b>subsequence</b> is any in-order subset of the sequence.  If a subsequence has a limit we call it at subsequential limit of the larger sequence.
        </p>
        <h3>Theorem: Sequences in compact sets always have a subsequential limit in the set. Every bounded sequence in has a subsequential limit.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> There are two important parts of this claim.  First, these sequences always have subsequential limits.  Second, that the limit is in the set.  Boundedness would already be enough to guarantee that the sequence has some subsequential limit.  Closedness guarantees that the set contains its limit points.
          </p>
          <p>
            First notice that, as soon as the first part is done we can quickly handle the second by embedding any bounded set in a <i>k</i>-cell.
          </p>
          <p>
            Now to prove the first part, we consider whether the sequence visits finitely or infinitely many points.  If it's finite the matter is trivial: Some point must be visited infinitely often since there are infinitely many indices.
          </p>
          <p>
            If it visits infinitely many points, then this sounds too much like previous theorems about infinite subsets and compactness.  Once you obtain a limit point it is easy to use it to construct a sequence approaching it.
          </p>
        </div>
      </div>
      <div>
        <h3>Theorem: The set of subsequential limits of a sequence is closed.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> The proof is direct from the definition of closed.  We select a point in the set and show that it is a limit point.  We have to care for some details, but the essential plan is to form a sequence of neighborhoods around the point which shrink to zero, picking elements of the sequence along the way.
          </p>
        </div>
      </div>
    </div>
    <h2>Section 2</h2>
    <h2>Cauchy Sequences</h2>
    <div>
      <p>
        A sequence is <b>Cauchy</b> if \((\forall \varepsilon\in\mathbb R^+)(\exists N\in \mathbb Z^+)(\forall n\in \mathbb Z^+,n\geq N)(d(p_n,p_m)< \varepsilon)\).  This effectively means the points get infinitely close to each other.  In a sense, convergence specifies what the limit point is; on the other hand, Cauchy does not name a point to which the sequence converges. Note also that there may not be a limit point.  You can have Cauchy sequences of rational numbers which "converge to a real", although if viewed entirely within the rationals then you can't actually speak about the reals.  So the more faithful statement would be that there are Cauchy sequences of rationals which do not have any limit.
      </p>
      <p>
        We call the <b>diameter</b> the sup over all the distances in a set.  Viewed geometrically in the plane it's like taking the smallest circle that encompasses all the points, and using its diameter.
      </p>
      <p>
        Unpacking definitions it is not a hard proof that a sequence is Cauchy if and only if the limit of the truncated sequence diameters goes to 0.  Said a little formally, a sequence is Cauchy if and only if

        $$ \displaystyle \lim_{N\rightarrow \infty} diam E_N = 0 $$

        where \(E_N\)  is the set of all points in the sequence after index <i>N</i>.
      </p>
      <div>
        <h3>Theorem: Taking the closure of a set doesn't change its diameter.  Any descending chain of compact sets with diameter approaching 0 consists of exactly one point.</h3>
        <div class="sol">
          <p>
              <i>Expalantion: </i> It's clear that adding more points can only increase the diameter, i.e. \( diam E \leq  diam \overline E\).  To prove the other direction, we take any two points in the closure and show and show that their distance cannot be more than diam<i>E</i>. In particular we show that they are each as close as you want to points in <i>E</i>, so you can find an upper bound on diam\(\overline E\) in terms of \(\varepsilon\) and diam<i>E</i>.  Since \(\varepsilon\) can be chosen arbitrarily small this implies \( diam\overline E\leq diam E \).
          </p>
          <p>
            <i>That descending chains of compact sets with diameters converging to zero have a unique element:</i> We already know that the intersection contains a point, so all that remains is to show it's unique with the new condition that the diameters go to zero.  But if there were two elements the diameter couldn't be smaller than their distance.
          </p>
        </div>
      </div>
      <div>
        <h3>
          Theorem: Convergence entails Cauchy.  In a compact space, Cauchy entails convergence.
        </h3>
        <div class="sol">
          <p>
            <i>Explanation:</i> Of course we are meant to think especially of the case of Euclidean spaces.  Since these are compact, this theorem shows that convergence and Cauchy are equivalent in Euclidean spaces.
          </p>
          <p>
            <i>That convergence entails Cauchy:</i> For any \(\varepsilon\) you can keep all the terms this close to the limit, and therefore close to each other.
          </p>
          <p>
            <i>That Cauchy entails convergence in compact spaces:</i> The the theorems just above will be useful.  In particular, we will use the idea of limits of diameters going to zero.  Since the image of the sequence may not be closed, notice that taking the closure of them doesn't change the diameter.  Now we have a descending chain of closed sets in a compact space.  We then know there is a unique element in the intersection, which we can prove is the limit point.
          </p>
        </div>
        <p>
          A space in which every Cauchy sequence converges is <b>complete</b>.
        </p>
        <p>
          A sequence is <b>monotonically increasing</b> if each next term is no less than the last.
        </p>
        <div>
          <h3>Theorem: A monotonically increasing sequence converges if and only if it's bounded.</h3>
          <div class="sol">
            <p>
              <i>Explanation:</i> If it converges we already know it's bounded, so the interesting part is the other direction.  Suppose it's bounded, then you can prove it converges to its supremum.
            </p>
          </div>
        </div>
      </div>
    </div>
    <h2>Section 3</h2>
    <h2>Upper and Lower Limits</h2>
    <div>
      <p>
        The <b>limsup</b> of a sequence is the supremum over the set of all subsequential limits.  This is also called the <b>upper limit</b>.  Similarly for <b>liminf</b>, called the <b>lower limit</b>.  These are respectively often denoted as \(s^*\) and \(s_*\).  (Note some authors define the limsup as the limit of the suprema of all truncated sequences.  These definitions are equivalent and proving this fact is a good exercise.)
      </p>
      <h3>Theorem: The limsup is a subsequential limit, and any point larger than the limsup is an upper bound for some truncated sequence.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> If the sequence is unbounded you can make a subsequence going to infinity in the usual way.  If it's bounded then we already know that the set of subsequential limits is a closed set.  The limsup is a limit point of this set and therefore in the set.
        </p>
        <p>
          For the second part, if we have a value larger than the sup then all the limits are strictly below it and eventually the sequences stay underneath.  For a proof, suppose some limit never stays below <i>x</i>, a contradiction for \(s^*\) comes pretty quickly thereafter.
        </p>
      </div>
      <h3>Theorem: If one sequence is above another, the lower and upper limits preserve the ordering.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Suppose we discuss the upper limits.  Argue that the upper limit of the bigger sequence is an upper-bound on all possible subsequential limits of the lower sequence.
        </p>
      </div>
    </div>
    <h2>Section 4</h2>
    <h2>Some Special Sequences</h2>
    <div>
      <h3>Theorem: If a sequence is squeezed between 0 and a sequence that goes to 0, then it goes to 0.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Rudin calls this a remark but we probably want to be able to prove it.  Note

          $$ |x_n-0| < \varepsilon \quad \Leftrightarrow \quad x_n < \varepsilon $$

          under these assumptions.  A similar fact can be said about the bounding sequence, and we know we can always fit that under any \(\varepsilon\).
        </p>
      </div>
      <h3>Theorem: \(\frac{1}{n^p}=0\) goes to 0 if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i>

          $$ \frac{1}{n^p} < \varepsilon \quad \Leftrightarrow \quad \frac{1}{\varepsilon^{1/p}} < n $$
        </p>
      </div>
      <h3>Theorem: \(\displaystyle \lim_{n\rightarrow \infty}\sqrt[n]{p} = 1\) if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Show that \(\displaystyle \lim_{n\rightarrow\infty} (p^{1/n}-1) = 0\).  After distributing limits, the result follows immediately.  And note that this is a common tactic in proving that a limit is this or that value: Convert the value itself into a limit and then merge limits.
        </p>
        <p>
          To show that \(\displaystyle\lim_{n\rightarrow\infty}(p^{1/n}-1)=0\) we use the theorem at the start of the section.  What will we use to bound this above?  This has a visible similarity to the binomial equation

          $$ 1+a+a^2+...+a^n = \frac{a^{n+1}-1}{a-1} $$

          especially in the numerator.  Actually it turns out we don't want the numerator equal to \(p^{1/n}-1\).  If you try it things get ugly.  But maybe something valuable happens if we take the denominator \(a-1=p^{1/n}-1\) so that we use the assignment \(a=p^{1/n}\).  Then

          $$ 1 + p^{1/n} + p^{2/n} + \dots + p^{1} = \frac{p^{(1/n)(n+1)}-1}{p^{1/n}-1} $$

          $$ = \frac{ p^{1+1/n}-1 } { p^{1/n}-1 } $$
        </p>
      </div>

    </div>






























  </body>
</html>

<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes - Rudin Principles Ch3</title>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../notes.css">
  </head>
  <body>
    <h1>Chapter 3</h1>
    <h1>Numerical Sequences and Series</h1>

    <h2>Section 1</h2>
    <h2>Convergent Sequences</h2>

    <div>
      <p>
        A sequence \(\{p_n\}\) is said to <b>converge</b> if there is a point \(p\in X\) such that \((\forall \varepsilon\in \mathbb R^+) (\exists N\in \mathbb Z^+) (\forall n\in \mathbb{Z}^+, n \geq N)( d(p_n,p) < \varepsilon)\).  We write either \(p_n\rightarrow p\) or \(\displaystyle \lim_{n\rightarrow \infty}p_n = p\).
      </p>
      <h3>Theorem: A sequence converges to a point if and only if every neighborhood of the point excludes only finitely many points in the sequence.  Limits are unique.  Convergence implies bounded.  Limit points of sets have sequences converging to the point, with the sequence in the set.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Proving the first sentence, we assume a sequence converges.  Then for each neighborhood with a given radius, there exists a corresponding <i>N</i>.  By definition only the terms before this could be outside the neighborhood.  For the reverse direction, the argument is similar.
        </p>
        <p>
          <i>That limits are unique:</i> Take any two limits and show that their distance from each other is smaller than every positive \(\varepsilon\).  To do this, find an <i>N</i> corresponding to \(\varepsilon/2\) for each limit point and use the triangle inequality.
        </p>
        <p>
          <i>That convergence implies bounded:</i> Use \(\varepsilon = 1\).  The terms after this are all less than \(p+\varepsilon\), thinking of a positive example (negatives are handled just as easily).  Terms before it are finite and must have a max.
        </p>
        <p>
          <i>That limit points of sets have a sequence in the set which converges to it:</i> Take a neighborhood, pick a point, take a smaller neighborhood ... Construct your sequence this way.
        </p>
      </div>
      <h3>Theorem: If individual limits exist then limits are linear, distribute over multiplication, and into denominators (except limit 0).</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> For distributing over sums, pick \(\varepsilon/2\) and use the triangle inequality.  For distributing over constants, use \(\varepsilon/c\).  Distributing over products is harder, so we focus on that.
        </p>
        <p>
          <i>That limits distribute over products:</i> Wiggle into \(|s_nt_n - st|\) the term \(s_nt\).  Then we need a bound on <i>n</i> which guarantees

          $$ |s_nt_n - s_nt + s_nt t_n - st| < \varepsilon $$
        </p>
        <p>
          Use the triangle inequality

          $$ |s_n t_n - s_n t + s_n t - st| \leq |s_nt_n - s_n t| + |s_n t - st| $$
          $$ = |s_n||t_n-t| + |t||s_n-s| $$
        </p>
        <p>
          Now we can easily get a bound on the right-hand term because <i>t</i> is fixed.  We can enforce a condition on the <i>s</i>'s in terms of \(\varepsilon\) and <i>t</i>.  The tricky part is the left-hand term because \(|s_n|\) is not fixed, it's a function of <i>n</i>.  When using the convergence of \(s_n\) we always need bounds that are constant in <i>n</i>.
        </p>
        <p>
          So the trick is to round it up to its maximum, since we already know it's bounded.
        </p>
        <p>
          <i>That limits distribute to the denominator: </i> We need to show \(\Big| \frac 1 {s_n}  - \frac 1 s \Big| < \varepsilon\).

          $$ \Big| \frac{1}{s_n} - \frac 1 s \Big| = \Big| \frac{s-s_n}{ss_n}\Big| $$

          And since the numerator can be bounded, that part is easy.  And the <i>s</i> in the denominator is constant, so not a worry.  But again the \(s_n\) is a cause for concern but this time it's in the denominator.  And rounding up to the upper bound won't help since it moves the inequality in the wrong direction.
        </p>
        <p>
          But actually it is somewhat clear that these old strategies won't work this time.  We need to use the assumption that the limit is not zero.  So find an <i>N</i> such that the sequence stays far enough away from zero.  For specificity, let <i>N</i> be such that \((\forall n\geq N)(|p_n-p| < p/2) \).  This again assumes <i>p > 0</i> but the proof for a negative <i>p</i> is very similar.  What we're effectively doing here is just choosing the half-way point between 0 and <i>p</i>.  With all the terms staying inside that region, we can then replace \(p_n\) with the lower bound <i>p/2</i>.
        </p>
      </div>
      <div>
        <h3>Theorem: Vector sequences converge if and only if each coordinate does.  They also obey distribution in sums and dot-products.  Also if \(\vec x_n \rightarrow \vec x, \beta_n \rightarrow \beta\) then \(\displaystyle \lim_{n\rightarrow \infty} \beta_n\vec x_n = \beta\vec x\).</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> If every coordinate convergences, then to make \(|\vec x_n - \vec x | < \varepsilon \) that means we need

            $$ \sqrt{(x_{1,n}-x_1)^2 + \dots + (x_{n,n}-x_n)^2} < \varepsilon $$
          </p>
          <p>
            We want to do a term-by-term rounding up of the LHS, and we want to replace each \(x_{i,n}-x_i\) by \(\varepsilon\) or at least something close (maybe divided by <i>n</i>, or after toying with that it might be even better to use \(\sqrt n\).)  Lucky for us the assumption is just right for this sort of thing.
          </p>
          <p>
            To go in the other direction is even easier just because the absolute value of any coordinate is always smaller than the absolute value of a vector.
          </p>
          <p>
            This then allows us to easily prove the rest coordinate-wise.
          </p>
        </div>
      </div>
      <div>
        <p>
          A <b>subsequence</b> is any in-order subset of the sequence.  If a subsequence has a limit we call it at subsequential limit of the larger sequence.
        </p>
        <h3>Theorem: Sequences in compact sets always have a subsequential limit in the set. Every bounded sequence in has a subsequential limit.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> There are two important parts of this claim.  First, these sequences always have subsequential limits.  Second, that the limit is in the set.  Boundedness would already be enough to guarantee that the sequence has some subsequential limit.  Closedness guarantees that the set contains its limit points.
          </p>
          <p>
            First notice that, as soon as the first part is done we can quickly handle the second by embedding any bounded set in a <i>k</i>-cell.
          </p>
          <p>
            Now to prove the first part, we consider whether the sequence visits finitely or infinitely many points.  If it's finite the matter is trivial: Some point must be visited infinitely often since there are infinitely many indices.
          </p>
          <p>
            If it visits infinitely many points, then this sounds too much like previous theorems about infinite subsets and compactness.  Once you obtain a limit point it is easy to use it to construct a sequence approaching it.
          </p>
        </div>
      </div>
      <div>
        <h3>Theorem: The set of subsequential limits of a sequence is closed.</h3>
        <div class="sol">
          <p>
            <i>Explanation: </i> The proof is direct from the definition of closed.  We select a point in the set and show that it is a limit point.  We have to care for some details, but the essential plan is to form a sequence of neighborhoods around the point which shrink to zero, picking elements of the sequence along the way.
          </p>
        </div>
      </div>
    </div>
    <h2>Section 2</h2>
    <h2>Cauchy Sequences</h2>
    <div>
      <p>
        A sequence is <b>Cauchy</b> if \((\forall \varepsilon\in\mathbb R^+)(\exists N\in \mathbb Z^+)(\forall n\in \mathbb Z^+,n\geq N)(d(p_n,p_m)< \varepsilon)\).  This effectively means the points get infinitely close to each other.  In a sense, convergence specifies what the limit point is; on the other hand, Cauchy does not name a point to which the sequence converges. Note also that there may not be a limit point.  You can have Cauchy sequences of rational numbers which "converge to a real", although if viewed entirely within the rationals then you can't actually speak about the reals.  So the more faithful statement would be that there are Cauchy sequences of rationals which do not have any limit.
      </p>
      <p>
        We call the <b>diameter</b> the sup over all the distances in a set.  Viewed geometrically in the plane it's like taking the smallest circle that encompasses all the points, and using its diameter.
      </p>
      <p>
        Unpacking definitions it is not a hard proof that a sequence is Cauchy if and only if the limit of the truncated sequence diameters goes to 0.  Said a little formally, a sequence is Cauchy if and only if

        $$ \displaystyle \lim_{N\rightarrow \infty} diam E_N = 0 $$

        where \(E_N\)  is the set of all points in the sequence after index <i>N</i>.
      </p>
      <div>
        <h3>Theorem: Taking the closure of a set doesn't change its diameter.  Any descending chain of compact sets with diameter approaching 0 consists of exactly one point.</h3>
        <div class="sol">
          <p>
              <i>Expalantion: </i> It's clear that adding more points can only increase the diameter, i.e. \( diam E \leq  diam \overline E\).  To prove the other direction, we take any two points in the closure and show and show that their distance cannot be more than diam<i>E</i>. In particular we show that they are each as close as you want to points in <i>E</i>, so you can find an upper bound on diam\(\overline E\) in terms of \(\varepsilon\) and diam<i>E</i>.  Since \(\varepsilon\) can be chosen arbitrarily small this implies \( diam\overline E\leq diam E \).
          </p>
          <p>
            <i>That descending chains of compact sets with diameters converging to zero have a unique element:</i> We already know that the intersection contains a point, so all that remains is to show it's unique with the new condition that the diameters go to zero.  But if there were two elements the diameter couldn't be smaller than their distance.
          </p>
        </div>
      </div>
      <div>
        <h3>
          Theorem: Convergence entails Cauchy.  In a compact space, Cauchy entails convergence.
        </h3>
        <div class="sol">
          <p>
            <i>Explanation:</i> Of course we are meant to think especially of the case of Euclidean spaces.  Since these are compact, this theorem shows that convergence and Cauchy are equivalent in Euclidean spaces.
          </p>
          <p>
            <i>That convergence entails Cauchy:</i> For any \(\varepsilon\) you can keep all the terms this close to the limit, and therefore close to each other.
          </p>
          <p>
            <i>That Cauchy entails convergence in compact spaces:</i> The the theorems just above will be useful.  In particular, we will use the idea of limits of diameters going to zero.  Since the image of the sequence may not be closed, notice that taking the closure of them doesn't change the diameter.  Now we have a descending chain of closed sets in a compact space.  We then know there is a unique element in the intersection, which we can prove is the limit point.
          </p>
        </div>
        <p>
          A space in which every Cauchy sequence converges is <b>complete</b>.
        </p>
        <p>
          A sequence is <b>monotonically increasing</b> if each next term is no less than the last.
        </p>
        <div>
          <h3>Theorem: A monotonically increasing sequence converges if and only if it's bounded.</h3>
          <div class="sol">
            <p>
              <i>Explanation:</i> If it converges we already know it's bounded, so the interesting part is the other direction.  Suppose it's bounded, then you can prove it converges to its supremum.
            </p>
          </div>
        </div>
      </div>
    </div>
    <h2>Section 3</h2>
    <h2>Upper and Lower Limits</h2>
    <div>
      <p>
        The <b>limsup</b> of a sequence is the supremum over the set of all subsequential limits.  This is also called the <b>upper limit</b>.  Similarly for <b>liminf</b>, called the <b>lower limit</b>.  These are respectively often denoted as \(s^*\) and \(s_*\).  (Note some authors define the limsup as the limit of the suprema of all truncated sequences.  These definitions are equivalent and proving this fact is a good exercise.)
      </p>
      <h3>Theorem: The limsup is a subsequential limit, and any point larger than the limsup is an upper bound for some truncated sequence.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> If the sequence is unbounded you can make a subsequence going to infinity in the usual way.  If it's bounded then we already know that the set of subsequential limits is a closed set.  The limsup is a limit point of this set and therefore in the set.
        </p>
        <p>
          For the second part, if we have a value larger than the sup then all the limits are strictly below it and eventually the sequences stay underneath.  For a proof, suppose some limit never stays below <i>x</i>, a contradiction for \(s^*\) comes pretty quickly thereafter.
        </p>
      </div>
      <h3>Theorem: If one sequence is above another, the lower and upper limits preserve the ordering.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Suppose we discuss the upper limits.  Argue that the upper limit of the bigger sequence is an upper-bound on all possible subsequential limits of the lower sequence.
        </p>
      </div>
    </div>
    <h2>Section 4</h2>
    <h2>Some Special Sequences</h2>
    <div>
      <h3>Theorem: If a sequence is squeezed between 0 and a sequence that goes to 0, then it goes to 0.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Rudin calls this a remark but we probably want to be able to prove it.  Note

          $$ |x_n-0| < \varepsilon \quad \Leftrightarrow \quad x_n < \varepsilon $$

          under these assumptions.  A similar fact can be said about the bounding sequence, and we know we can always fit that under any \(\varepsilon\).
        </p>
      </div>
      <h3>Theorem: \(\frac{1}{n^p}=0\) goes to 0 if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i>

          $$ \frac{1}{n^p} < \varepsilon \quad \Leftrightarrow \quad \frac{1}{\varepsilon^{1/p}} < n $$
        </p>
      </div>
      <h3>Theorem: \(\displaystyle \lim_{n\rightarrow \infty}\sqrt[n]{p} = 1\) if <i>p > 0</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Show that \(\displaystyle \lim_{n\rightarrow\infty} (p^{1/n}-1) = 0\).  After distributing limits, the result follows immediately.  And note that this is a common tactic in proving that a limit is this or that value: Convert the value itself into a limit and then merge limits.
        </p>
        <p>
          To show that \(\displaystyle\lim_{n\rightarrow\infty}(p^{1/n}-1)=0\) we use the theorem at the start of the section.  What will we use to bound this above?  It helps to just write out \(x_n = p^{1/n}-1\) and it is not a crazy idea to solve for <i>p</i>, although I honestly can't motivate this move any better than to just say it's "not crazy".  I guess you just play with it long enough until you stumble onto this.
        </p>
        <p>
          Once you solve for <i>p</i>, you notice that the binomial theorem applies, and we use the approximation that we discovered in the first chapter.

          $$ (1+x_n)^n \geq 1+nx_n $$
        </p>
      </div>
      <h3>Theorem: \(\sqrt[n] n \rightarrow 1\)</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Similar to the previous problem, except when you get to the application of the binomial theorem. The familiar approximation no longer works.  However, we can still approximate it as an upper bound on the second-order term.  (Since every term is zero and so we can drop any that we want.)
        </p>
      </div>
      <h3>Theorem: \(\frac{n^\alpha}{(1+p)^n} \rightarrow 0\) if <i>p > 0</i> and \(\alpha\in\mathbb R\).</h3>
      <div class="sol">
        <p>
          <i>Explanation: </i> This time we don't solve for anything per se, but we can just directly approximate \((1+p)^n\) which is intuitively the hard spot in this problem anyway.  And of course this is also intuitive because it already looks like the binomial theorem.
        </p>
        <p>
          As we saw before we can approximate it down by dropping any terms we want.  If we keep only the <i>k</i>th term let's see what we get.

          $$ \frac{n^\alpha}{(1+p)^n} < \frac{n^\alpha}{\binom n k p^k} $$
        </p>
        <p>
          We can approximate that down even further by approximating up the binomial factor.

          $$ \binom n k = \frac{n!}{(n-k)!k!} = $$
          $$ n(n-1)\cdots (n-k+1)/k! $$

          Clearly we can round this down by changing <i>n-i</i>s to 1s but this approximation, if you try it, is too severe.  The proof will typically fail.  One thought is that we can approximate at least half of them down to <i>n-i</i> to <i>n/2</i> so long as <i>n-i > n/2</i> (which of course, switches from true to false about halfway down the list of factors).  This helps guide us in the choice of <i>k</i>.  We need <i>n/2 > k</i>.  For any such choice of <i>k</i>

          $$ \binom n k > \frac{n^k}{2^kk!} $$

          This effectively rounds the first <i>k</i> factors to <i>n/2</i> and the rest to 1.
        </p>
        <p>
          Then we have

          $$ \frac{n^\alpha}{(1-p)^n} < \frac{n^\alpha}{n^kp^k/(2^kk!)} $$
          $$ = \frac{n^{\alpha-k}2^kk!}{p^k} $$

          If we in particular pick \(\alpha-k < 0\) then \(n^{\alpha-k}\rightarrow 0\) by an earlier theorem.  We know we can pick a <i>k</i> large enough for this, and then pick an <i>n</i> large enough for <i>n/2 > k</i>.
        </p>
      </div>
      <h3>Theorem: \(x^n\rightarrow 0\) if |<i>x</i>| < 1.</h3>
      <div class="sol">
        <p>
          <i>Explanation: </i> Use the theorem above.  With \(\alpha = 0\) we can make any \(x\geq 0\) equal to some \(\frac 1 {1+p}\).  For the case where <i>x < 0</i>, the even terms and the odd terms both go to 0.  So for any \(\varepsilon\in\mathbb R^+\) we can find an <i>N</i> such that for every odd \(n\geq N \) we have \(|x^n| < \varepsilon\) and then this automatically also holds for all even terms after <i>N+1</i>.  So it holds for all \(n\geq N+1\).
        </p>
      </div>
    </div>
    <h2>Section 5</h2>
    <h2>Series</h2>
    <div>
      <p>
        We did all that work on sequences, in part, because we want to take infinite sums (i.e. <b>series</b>) which are defined as the limits of partial sums.  Each <b>partial sum</b> is just the sum up to some finite final term.  The limit is taken over the number of terms.  If the sequence of terms is given by \(\{a_n\}\) then a partial sum up to <i>q</i> is

        $$ \sum_{n=p}^q a_n $$

        for the series starting at <i>p</i>.  Then for a series

        $$ \sum_{n=1}^\infty a_n = \lim_{q\rightarrow\infty} \sum_{n=1}^q a_n $$

        We say that the series converges or diverges if the limit does.
      </p>
      <p>
        Particularly useful is the Cauchy criterion for series, which becomes:

        $$ (\forall \varepsilon\in\mathbb R^+)(\exists N\in\mathbb Z^+)(\forall m,n\in\mathbb Z^+, \text{ and } m,n\geq N)\left(\left|\sum_{k=m}^n a_k\right|<\varepsilon\right) $$
      </p>
      <h3>Theorem: Test for Divergence</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The test for divergence is the theorem that, if a series converges, then the limit of the sequence of terms goes to 0.  That is to say

          $$  \sum_{n=1}^\infty a_n < \infty \Longrightarrow a_n\rightarrow 0 $$

          This is a good name for the theorem because it is a quick and easy way to show that some series diverge (using the contrapositive).  Whenever meeting a new series your first question should be "does the limit of terms go to 0?"  Much of the time the answer is yes, and you've learned nothing.  But in a good number of cases the answer will be no, and you will be able to immediately infer that the series diverges.
        </p>
        <p>
          The proof follows immediately from the Cauchy criterion for converges when selecting <i>m = n</i>.
        </p>
      </div>
      <h3>Theorem: Non-negative Convergence Test.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The non-negative convergence test is a theorem that applies to series of non-negative terms.  For such a series, it must converge if and only if the partial sums are bounded. If the series converges then the partial sums are bounded just from the fact that convergent sequencecs are bounded.  If the partial sums are bounded then they form an increasing (because the terms are non-negative) bounded sequence, and so must converge.
        </p>
      </div>
      <h3>Theorem: Comparison Test.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The comparison test says that if a larger series converges, then the smaller one does; and if a smaller series diverges then the larger one does.  More rigorously, we imagine some given series \(\displaystyle \sum_{n=1}^\infty a_n\).  If \(|a_n|\leq c_n\) for some other terms \(c_n\) and some \(N_0\) and each \(n\geq N_0\), then

          $$ \sum_{n=1}^\infty c_n < \infty \Longrightarrow \sum_{n=1}^\infty a_n < \infty $$
        </p>
        <p>
          It has a dual as well. If a smaller series diverges then the larger one does.  That is to say, if \(a_n\geq d_n\geq 0\) for \(n\geq N_0\), then

          $$ \sum_{n=1}^\infty d_n \not\in \mathbb R \Longrightarrow \sum_{n=1}^\infty a_n \not\in \mathbb R $$
        </p>
        <p>
          The proof of the first part just uses the Cauchy criterion and the triangle inequality on partial sums.  The proof for the second part simply applies the contrapositive to the first part.
        </p>
      </div>
      <h2>Section 6</h2>
      <h2>Series of Nonnegative Terms</h2>
      <h3>Theorem: The Geometric Series</h3>
      <div class="sol">
        <p>
          The geometric series is \(\displaystyle\sum_{n=1}^\infty x^n\) and famously if \( 0\leq x < 1\) then the series equals

          $$ \frac{1}{1-x} $$

          Moreover if \(x\geq 1\) the series diverges.
        </p>
        <p>
          The proof of convergence when \(0\leq x < 1\) is an easy consequence of the finite geometric sum

          $$ \sum_{n=1}^m x^n = \frac{1-x^{m+1}}{1-x} $$
        </p>
        <p>
          If you're interested in how <i>that</i> is proved, by the way, it is a direct confirmation that only the first and last terms of

          $$ (1-x)(1+x+x^2+\dots +x^n) $$

          don't cancel with anything.
        </p>
        <p>
          The proof of divergence when <i>x = 1</i> is trivial.  Of course the comparison test tells us that if we make these terms any larger then it still diverges.  That covers the case when \(x \geq 1\).
        </p>
      </div>
      <h3>Theorem: Cauchy's Covering Theorem (my name for it, not recognized by ... anyone else).</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The theorem is that, for a decreasing sequence of non-negative \(a_n\),

          $$ \sum_{n=1}^\infty a_n < \infty \Longleftrightarrow \sum_{n=0}^\infty 2^na_{2^n} < \infty $$

          which seems like an utterly useless theorem.  At least at first glance it seemed that way to me--why should we care about this powers of two thing?  Its use is that it affords a kind of severe rounding of the terms to just the value at every power of two, which is incredibly sparse in the integers.
        </p>
        <p>
          The main trick to the proof is to group the terms into groups of size each power of 2.  So group the first term, and then terms 2 through 3, then 4 through 7, and so on.
        </p>
        <p>
          If the series converges then every group can be rounded down to \(2^na_{2^n}\) because the sequence is decreasing and \(a_{2^n}\) is the start of each group, so larger than the other terms in it.
        </p>
        <p>
          The harder direction is to prove the converse.  In fact Rudin doesn't even prove the first direction because his proof is so much nicer.  Because the two sequences \(a_n\) and \(2^na_{2^n}\) are positive and decreasing, we can rely on the theorem which tells us they converge if and only if the partial sums are bounded.  In particular, the really sweet flavor of Rudin's proof comes from the fact that we can prove that they are either both bounded or both unbounded.  Then they either converge together or diverge together.
        </p>
        <p>
          More specifically the proof strategy is to show that for any \(\displaystyle \sum_{n=1}^m a_n\) we can find a \(\displaystyle\sum_{n=0}^k 2^n a_{2^n}\) which is bigger.  And conversely for each partial sum of the second kind we can find a sum of the first kind which is bigger.  Thus if all partial sums of one is bounded, it's also a bound for the other.
        </p>
        <p>
          Showing that a sum of the first kind is bounded by some sum of the second is done in the same way as my proof earlier.  Showing that a sum of the second kind is bounded by a sum of the first kind ... well I lied a little bit.  That's close but not quite what we will show.
        </p>
        <p>
          For any given sum of the second kind, we expand each term, as a kind of inverse of the collapsing that we did in the left-to-right direction.  It might be best illustrated on a small example.  If you have \(a_1+2a_2+4a_4+8a_8\) then we know we want to line this up with terms from a series of the first kind.  Thinking of \(4a_4 = a_4+a_4+a_4+a_4\) we will want to round one of these to \(a_3\) and one to \(a_2\) and one to \(a_1\). Then use the replacement \(4a_4 \leq a_1+a_2+a_3+a_4\).  But what do we do with the \(a_2\) and \(a_1\) that we started with?  We seem to have reproduced them, and they don't fit neatly into something that will give us a series of the first kind.
        </p>
        <p>
          There are two ways to see the resolution.  You could try to see how we could do the expansion and find that every term is re-produced exactly twice. But easier would be to see that we don't really want from \(a_4\) four terms, we just want two.  And from \(a_8\) we jsut want four.  We can easily represent this by factoring out a 2, so that

          $$ \sum_{n=1}^k 2^na_{2^n} = 2\sum_{n=1}^k 2^{n-1}a_{2^n} $$
          $$ = 2\left(\frac{1}{2} a_1 + a_2 + 2a_4 + \dots + 2^{k-1}a_{2^{k-1}}\right) $$
          $$ \leq 2(a_1+ a_2 + (a_3+a_4) + (a_5+a_6+a_7+a_8)+\dots+(a_{2^{k-1}+1}+\dots+a_{2^k}) $$
          $$ = 2\sum_{n=1}^{2^k}a_n $$
        </p>
      </div>
      <h3>Theorem: <i>p</i>-series.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> The theorem claims that

          $$ \sum \frac 1 {n^p} $$

          converges if <i>p > 1</i> and diverges otherwise.  The "otherwise" case is easily handled by the test for divergence.  When <i>p < 1</i>  we apply the previous theorem.

          $$ \sum 2^k \cdot \frac 1 {2^{kp}} = \sum 2^{k(1-p)} $$
        </p>
        <p>
          And from here it is easy to argue that this is a geometric series with common ratio less than 1.
        </p>
      </div>
      <h3>Theorem: \(\displaystyle \sum \frac 1 {n(\log n)^p} \) converges just if <i>p > 1</i>.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> Again use Cauchy's covering theorem, which interacts nicely with the logarithm.
        </p>
      </div>
      <div>
        <p>
          We define \(\displaystyle e = \sum_{n=0}^\infty \frac 1 {n!}\) for no apparent reason.
        </p>
        <h3>Theorem: \(\left( 1+\frac 1 n\right)^n \rightarrow e \)</h3>
        <div class="sol">
          <p>
            <i>Explanation:</i> It is easy to show that \(\displaystyle\left(1+\frac 1 n\right)^n \leq \sum_{m=1}^n \frac 1 {n!}\) merely with the binomial theorem, after expanding out the binomial function and grouping factors.  Since limsups preserve ordering, we have

            $$ \limsup_{n\rightarrow \infty} \left(1+\frac 1 n\right)^n \leq e $$
          </p>
          <p>
            Proving the reverse inequality is tricky.  The intuition is to play with the binomial expansion of \(\left(1+\frac 1 n\right)^n\) and observe that the non-factorial part is (1) the part we want to go away and (2) goes to 1 as we let the denominators go to infinity.  So for any fixed choice of <i>m</i> we can find an <i>n</i> such that

            $$ \left(1+\frac 1 n\right)^n \geq 1+1+\frac{1}{2!}\left(1-\frac 1 n\right)+\dots+\frac{1}{m!}\left(1-\frac 1 n\right)\left(1-\frac{m-1}{n}\right) $$

          </p>
          <p>
            And now we can take the limit on both sides as \(n\rightarrow\infty\) ... except we kind of can't.  If you're impulse is just to apply \(\displaystyle \lim_{n\rightarrow\infty}\) to both sides, keep in mind that we haven't proved this theorem.  And it's not true if one of the limits isn't already guaranteed to exist.  Instead we have to use the facts that limsups and liminfs preserve order.
          </p>
          <p>
            If you're tempted to apply the limsup, because did in the earlier part, and now you get equality ... well you're right.  But then you're left wondering how to show that the limsup is the limit.  If you applied the liminf instead, you would have a liminf above a limsup.  You know that implies equality.  And because the equality of liminf and limsup implies equality with the limit, now you're done.
          </p>
        </div>
      </div>
      <div>
        <p>
          We will later be interested in the speed with which sequences converge to their values, and we can see a first instance of this analysis with <i>e</i>.  We estimate \(\displaystyle e - \sum_{n=0}^m \frac{1}{n!}) by canceling the shared first <i>m </i> + 1 terms.

          $$ \sum_{n=m+1}^\infty \frac{1}{n!} = \frac{1}{(m+1)!} + \frac{1}{(m+2)!} + \dots  $$
          $$ = \frac{1}{(m+1)!}\left(1 + \frac{1}{m+2} + \frac{1}{(m+3)(m+2)} + \dots \right) $$
        </p>
        <p>
          I'm tempted to try to wrangle this into a form that looks like <i>e</i>.  Certainly the given quantity is less than

          $$ \frac{1}{(m+1)!}e $$

          and for the purpose of seeing how fast these "remainders" go to zero, this is good enough.  It goes to zero at least as fast as \(\frac{1}{n!}\) which is very fast.
        </p>
        <p>
          But a different approximation will be even more useful.  If we instead approximate

          $$ 1+\frac{1}{m+2}+\frac{1}{(m+3)(m+2)}+\dots < 1+\frac{1}{m+2}+\frac{1}{(m+2)^2}+\dots $$

          which is a less severe rounding anyway, then we get a geometric series.  The advantage of this is we get an express in rational numbers.

          $$ 0 < e-\sum_{n=0}^m \frac{1}{n!} < \frac{1}{(m+1)!}\cdot \frac{1}{1-1/(m+2)} $$
          $$ = \frac{1}{(m+1)!}\frac{1}{\frac{m+2-1}{m+2}} = \frac{1}{(m+1)!}\frac{m+2}{m+1} $$
        </p>
        <p>
          The advantage of a rational expression is that we can use this to prove <i>e</i> is irrational.  Before doing so it'll be nice to use an even simpler rational expression, in order to minimize computation later on.

          $$ \frac{1}{(m+1)!}\frac{m+2}{m+1} < \frac{1}{(m+1)!}\frac{m+1}{m} $$
          $$ =\frac{1}{m!m} $$
        </p>
      </div>
      <h3>Theorem: <i>e</i> is irrational.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> If <i>e = p/q</i> then we should get some kind of integer-like problem from

          $$ 0 < e-\sum_{n=0}^m \frac 1 {n!} < \frac{1}{m!m} $$
        </p>
        <p>
          Clearly we have that every quantity here is then rational--let's do something to make everything integers.  Let's multiply by <i>m!m</i>.

          $$ 0 < m!me - m!m\sum_{n=0}^m \frac 1 {n!} < 1 $$
        </p>
        <p>
          Since we want integers, and <i>m</i> is free for us to pick, then setting <i>m = q</i> implies that each of <i>q!qe = q!q(p/q)</i> and \(\displaystyle q!q\sum_{n=0}^q\frac{1}{n!}\) are integers.  So there's an integer between 0 and 1, a contradiction.
        </p>
      </div>
    </div>
    <h2>Section 7</h2>
    <h2>The Root and Ratio Tests</h2>
    <h3>Theorem: Root Test</h3>
    <div class="sol">
      <p>
        <i>
          Explanation:
        </i> The theorem claims that if \(\{a_n\}\) is a sequence, and the limsup of \(\sqrt[n]{|a_n|}\) is less than one, then \(\sum a_n < \infty\).  If it's > 1 then the series diverges, and the test is indeterminant when = 1.
      </p>
      <p>
        For the case when the limsup is < 1, this seems related to the geometric series.  We can in fact get powers of <i>n</i> by considering any \(0 < \beta < 1\) such that for some <i>N</i> and any \(n\geq N\)

        $$ \sqrt[n]{|a_n|} < \beta $$

        so that

        $$ |a_n| < \beta^n $$
      </p>
      <p>
        From here we can use the comparison test with a geometric series to easily show convergence.
      </p>
      <p>
        When the limsup is above 1, we know a subsequence stays above 1.  From here it's not too hard to show that the series fails the test for divergence.
      </p>
      <p>
        To prove that the test in indeterminant when the limsup is 1, we want to find two series, each with limsup equal to 1, but one convergent and the other not.  Consider the two <i>p</i>-series

        $$ \sum \frac 1 n, \quad \sum \frac{1}{n^2} $$
      </p>
    </div>
    <h3>Theorem: Ratio Test</h3>
    <div class="sol">
      <p>
        <i>
          Explanation:
        </i> The theorem claims that if the limsup of \(\left|\frac{a_{n+1}}{a_n}\right|\) is < 1, then \(\sum a_n < \infty\). If the liminf is > 1, then divergence.
      </p>
      <p>
        If the limsup is < 1, like before we can find a \(0 < \beta < 1\) and <i>N</i> such that for all \(n\geq N\)

        $$ \left|\frac{a_{n+1}}{a_n}\right| < \beta $$
      </p>
      <p>
        The main trick of the proof is to realize that every <i>n</i> gives an inequality, and these can be rearranged and chained together.

        $$ |a_{N+1}| < \beta |a_N| $$
        $$ |a_{N+2}| < \beta |a_{N+1}| < \beta^2|a_N| $$
      </p>
      <p>
        What you see is that every \(|a_{n}|\) can be related to \(|a_N|\).

        $$ |a_n| < |a_N|\beta^{-N}\cdot \beta^n $$

        Well this looks like a geometric series again, and we can use the comparison test.
      </p>
      <p>
        When the liminf is greater than 1, it's easy enough to show that the series fails the test for divergence.
      </p>
    </div>
    <h3>Theorem: For any sequence \(\{c_n\}\) of positive numbers,

            $$ \liminf_{n\rightarrow \infty}\frac{c_{n+1}}{c_n}\leq \liminf_{n\rightarrow\infty}\sqrt[n]{c_n} $$
            $$ \limsup_{n\rightarrow \infty}\sqrt[n]{c_n}\leq \limsup_{n\rightarrow\infty}\frac{c_{n+1}}{c_n} $$
    </h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> Focusing on the second of these, if \(\displaystyle\limsup_{n\rightarrow \infty} \frac{c_{n+1}}{c_n}\) is infinte then there is nothing to prove.  If it's finite then suppose \(\beta\) is a larger value and therefore upper-bounds every ratio.  Do the same trick as in the proof of the ratio test.  Take the <i>n</i>th root, make an inference about the limsup.
      </p>
      <p>
        Since the above is true for every \(\beta > \limsup_{n\rightarrow\infty} \frac{c_{n+1}}{c_n} \) thus we infer the inclusive inequality.
      </p>
    </div>
    <h2>Section 8</h2>
    <h2>Power Series</h2>
    <div>
      <p>
        A <b>power series</b> is any \(\displaystyle\sum_{n=0}^\infty c_nz^n\) for a complex sequence \(\{c_n\}\).  Note that the series is not itself technically a polynomial.  It is a complex function and is defined by plugging in a complex <i>z</i> and seeing to what the corresponding series converges.
      </p>
      <h3>Theorem: Every power series has a radius of convergence.</h3>
      <div class="sol">
        <p>
          <i>Expalantion:</i> For any \(\sum c_nz^n \) the theorem claims that there is an extended real number <i>R</i> such that the series converges if <i>|z| < R</i>, diverges if <i>|z| > R</i>.  In particular

          $$ R = \frac 1 {\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}} $$
        </p>
        <p>
          The proof effectively swallows the entire \( c_nz^n \) into a single term \(a_n\) and applies the root test.
        </p>
      </div>
    </div>
    <h2>Section 9</h2>
    <h2>Summation by Parts</h2>
    <h3>Theorem: A finite sum of products satisfies a nice(-ish) recursive relationship.

      $$ \sum_{n=p}^q a_nb_n = \sum_{n=p}^{q-1} A_n(b_n-b_{n+1}) + A_qb_q - A_{p-1}b_p $$

      where \(A_n = \sum_{k=0}^n a_k\).
    </h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> The proof is very straight-forward

        $$ \sum_{n=p}^q a_nb_n = \sum_{n=p}^{q-1} (A_n-A_{n-1})b_n $$

        and then distribute and split of late and early terms.
      </p>
      <p>
        The expression treats \(\{b_n\}\) differently, especially introducing \(b_n-b_{n+1}\).  If the sequence is montonic we then always know the sign of this factor.
      </p>
    </div>
    <h3>Theorem: \(\sum a_nb_n < \infty\) when the partial sums of \(\sum a_n\) are bounded, when \(b_n\) is monotonically decreasing, and when \(b_n\rightarrow 0\).</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> The proof is direct from Cauchy convergence, and falls right out of the assumptions and the previous theorem.
      </p>
    </div>
    <h3>Theorem: Alternating Series Test</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> The theorem states that, if a sequence is monotonically decreasing in absolute value, alternating, and goes to 0, then the series always converges.
      </p>
      <p>
        The proof merely uses the previous result with \(a_n=(-1)^n\) and \(b_n=|c_n|\).
      </p>
    </div>
    <h3>Theorem: Convergence on the radius except at 1, for power series with decreasing coefficients that go to 0.</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> We already have the theorem about radii of convergence always existing, but we couldn't get a clear answer about convergence on the radius.  Here we get some clarity for a class of power series.  Namely if it's \(\sum c_n z^n\) and:

        <ul>
          <li>The radius is 1</li>
          <li>\(c_n\rightarrow 0\)</li>
          <li>\( c_1 \geq c_2 \geq \dots \)</li>
        </ul>

         then convergence of the power series holds for all numbers on the radius, i.e. for <i>|z| = 1</i>, except for possibly <i>z = 1</i>.  If the radius is not 1 we can
      </p>
      <p>
        The proof again merely leverages the earlier theorem.  Setting \(a_n = z_n\) and \(b_n = c_n\), we do need to see that the partial sums \(A_n\) are bounded before we can infer the result.

        $$ |A_n| = \left|\sum_{m=0}^n z^m\right| = \left|\frac{1-z^{n+1}}{1-z}\right| $$

        and if <i>|z| = 1</i> then \(|1-z^{n+1}| \leq |1|+|-z^{n+1}| \leq 2\).
      </p>
    </div>
    <h2>Section 9</h2>
    <h2>Absolute Convergence</h2>
    <div>
      <p>
        \(\sum a_n\) is said to <b>converge absolutely</b> if \(\sum|a_n|<\infty\).
      </p>
    </div>
    <h3>Theorem: Absolute convergence implies convergence.</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> This merely applies the Cauchy criterion and the triangle inequality.
      </p>
    </div>
    <h2>Section 10</h2>
    <h2>Addition and Multiplication of Series</h2>
    <h3>Theorem: If two series converge individually then they satisfy linearity.</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> This just comes from limits and linearity of finite sums.
      </p>
    </div>
    <div>
      <p>
        We define the <b>Cauchy product</b> of \(\sum a_n\) and \(\sum b_n\) to be \(\sum \sum a_kb_{n-k} \). We define it this way as the natural extension of how finite products of sums distribute.
      </p>
    </div>
    <h3>Theorem: If one series converges absolutely, and another series converges, then the series of the Cauchy product is the product of the series.</h3>
    <div class="sol">
      <p>
        <i>Explanation:</i> Say the partial sums of all three series are \(A_n,B_n,C_n\).  Take the terms of the Cauchy product, \(C_n\).  Group all the \(a_0\) terms together and get \(a_0B_n\).  Group the \(a_1\) terms, and so on, you get

        $$ C_n = a_0B_n+\dots+a_nB_0 $$
      </p>
      <p>
        If \(\sum a_n = A, \sum b_n = B\) then use the remainder of \(B_n\) defined by \(B = B_n - \beta_n\).  Replace \(B_n\) in the above and collect <i>B</i> so tht you get

        $$ A_n B + \sum a_n \beta_{n-k} $$
      </p>
      <p>
        Taking the limit as \(n\rightarrow\infty\) we hope the right-hand term dies. Because \(\sum b_n < \infty\) we know \(\beta_n \rightarrow 0\).  So at this point we jump into an \(\varepsilon, N\) proof to show

        $$ \sum a_n \beta_{n-k} = 0 $$

        Using the <i>N</i> that we get from the \(\beta\)s, we break the sum above into two parts.  One is rounded up to \(\varepsilon \sum|a_n|\) at which point the assumption of absolute convergence kicks in. The other term goes to 0 simply because \(a_k\rightarrow 0\).
      </p>
    </div>
    <h2>Section 11</h2>
    <h2>Rearrangements</h2>
    <div>
      <p>
        A rearrangement of \(\sum a_n\) is any \(\sum a_{\sigma(n)}\) where \(\sigma\) is a permutation of the natural numbers.
      </p>
      <h3>Theorem: Any convergent series which is not absolutely convergent can be rearranged to converge to anything.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> I'm not currently going to explain this because it's pretty elaborate for a theorem that isn't very useful really.  The most important point is just that you are warned against doing rearrangements on series that don't converge absolutely.  But the big-picture version of the proof is:  Separate the series out into its negative and positive terms.  Then take positive terms until you go over the value you want.  Then start taking negative terms until you go under.  Continue, and in the limit, this converges to whatever value you want.
        </p>
      </div>
      <h3>Theorem: Absolutely convergent series are invariant under rearrangements.</h3>
      <div class="sol">
        <p>
          <i>Explanation:</i> We show that the difference of the two sums goes to 0, which is to say

          $$ \sum(a_n-a_{\sigma(n)}) = 0 $$

          Using absolute convergence we get an <i>N</i>.  Go far enough out in the permutation so that all natural numbers up to <i>N</i> appear.
        </p>
      </div>
    </div>




















  </body>
</html>
